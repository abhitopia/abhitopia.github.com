{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"If you can\u2019t explain it simply, you don\u2019t understand it well enough. - Albert Einstein The purpose of this blog is three fold As a journal, so I can look back when I am 60 and remember what was it I wasted prime time of my life on. :) Organize plethora of information I deem worth preserving for purpose of knowledge sharing and recall. Learn better by following Feynman Technique Following topics are currently of interest to me Universe \u00b6 I do wild speculations and connect information and computation to the fundamental fabric of nature using analogies, thought experiments, and at times outragious leaps of faith. Mostly meant to organise to provide a medium to channel my thought and hopefully encourage some of my genious non-existent readers to develop this idea further. Tensor Algebra \u00b6 A short course (WIP) to build solid foundation in Tensor Algebra, which is a prerequisite to Tensor Calculus, which itself is a prerequisite to Differential Geometry, which in turn forms the mathematical language for Einstein's General Theory of Relativity. Machine Learning \u00b6 Different Machine learning topics, including an archive of interesting papers . Contact \u00b6 I am always looking for like-minded people and chances are, if you are reading this blog, you are one of them. If you are interested in giving feedback, provide new or discuss existing content, or just want to say hi, you can reach out to me at Email: abhi at abhitopia . com Twitter: abhitopia","title":""},{"location":"#universe","text":"I do wild speculations and connect information and computation to the fundamental fabric of nature using analogies, thought experiments, and at times outragious leaps of faith. Mostly meant to organise to provide a medium to channel my thought and hopefully encourage some of my genious non-existent readers to develop this idea further.","title":"Universe"},{"location":"#tensor-algebra","text":"A short course (WIP) to build solid foundation in Tensor Algebra, which is a prerequisite to Tensor Calculus, which itself is a prerequisite to Differential Geometry, which in turn forms the mathematical language for Einstein's General Theory of Relativity.","title":"Tensor Algebra"},{"location":"#machine-learning","text":"Different Machine learning topics, including an archive of interesting papers .","title":"Machine Learning"},{"location":"#contact","text":"I am always looking for like-minded people and chances are, if you are reading this blog, you are one of them. If you are interested in giving feedback, provide new or discuss existing content, or just want to say hi, you can reach out to me at Email: abhi at abhitopia . com Twitter: abhitopia","title":"Contact"},{"location":"ML/reasoning_fundamentals/","text":"Reasoning about nature of Reasoning \u00b6 This is messy idea soup, and not meant for public consumption Knowledge Representation \u00b6 Comparison to word embeddings which do not account for context. The idea is that words are just symbols and the context determines which entities they are mapped to. This is also the reason that current neural architectures are not interpretable because we do not form embeddings of relations but instead of symbols. In this new paradigm, we must form embeddings for relations and embeddings Reasoning \u00b6 Intelligent system \u00b6 Learning Intelligent system \u00b6 Knowledge is embedded in analogies or relations. Based on above premise, with enough text data, one shall be able to extract a pretty good (and intelligent) description of world and reason about it. All intelligent arguments can be inferred from basic building block relations and set of primitive entities. \u00b6 Knowledge is then a set of primitive entities and corresponding relations. This need not be a fully connected graph. A complete knowlege system is one, where one can reason about any two entities in the entity space. Reasoning is then defined as 1. input -> entity_1 2. relation(entity_2) -> entity_2 Properties of relations \u00b6 They are very much like in maths R(A, B) -> {true| false} R(A) -> B R^(B) -> A Also, given A, B -> one should be able to find the relation (important for interpretability) Note: this does not mean that R^ must exist as a primitive relation. A reasoning system must be able to construct all entities from the primitive entities using convex combination. Imagine a spiders web. If primitive entities are represented at the outer edges of the web, then all the space of new (non-primitive) entities can be created as a linear combination of primitive entities. Implementation ideas \u00b6 |E| -> |R| number of possible arguments can then be |R|^(number of steps of reasoning). ( Connection with group theory?? ) One can argue that given the amount of relations and entities, any argument can be made with reason. Reasoning is then an optimization process to find the shortest relation between two (new or primitive) entities. This idea can also be used to determine when to stop reasoning. Role of context (temporal information, and symbols) \u00b6 context is then used solely to map the symbols (or any input modality) into corresponding entities. Attributes of Learning system \u00b6 learning system must be able to learn primitive entities and relations. learn the mapping function learning system must be able to contruct new relations (to reduce the amount of computation) Once set of primitive relations and primitive entities are acquired, along with the mapping (symbol, context) -> entity mapping function, a learning system should be very sample efficient in acquiring new concepts. Learning a task \u00b6 is tantamount to forming more dense relations amount context/task dependent entities. A > B, B > C => A > C Entities in this respect are just variables. It is the job of mapping function to map symbols to these variables. Learning \u00b6 relations that most used can follow hebbian learning rule, and thus are not the first one to be overwritten. adding new relations (must take some computation), that is an analogy must be made. a new relation (think more) is a composition of premitive relations??? thus a same inference can be obtained by either more computation by applying consequtive compositional relations on entities. Or a shortcut may be formed or repeated application (hebbian learning again?) optimization \u00b6 use forward prop to form new relations? can forward prop be a one step optimization process to find the analogies? Role of memory? \u00b6 Can some emtities or relation be more active due to priming effect of forward processing? Fast Weights? Perhaps priming can activate all the entities and relations connected by few steps to the last used entities and relations. demo \u00b6 Theorem proving? Sample Efficiency Interpretabilty? also consider adding optimization techniques like \u00b6 relative position for text (transformer-xl) TPR can be a good choice for representing entities and relations","title":"Reasoning about nature of reasoning"},{"location":"ML/reasoning_fundamentals/#reasoning-about-nature-of-reasoning","text":"This is messy idea soup, and not meant for public consumption","title":"Reasoning about nature of Reasoning"},{"location":"ML/reasoning_fundamentals/#knowledge-representation","text":"Comparison to word embeddings which do not account for context. The idea is that words are just symbols and the context determines which entities they are mapped to. This is also the reason that current neural architectures are not interpretable because we do not form embeddings of relations but instead of symbols. In this new paradigm, we must form embeddings for relations and embeddings","title":"Knowledge Representation"},{"location":"ML/reasoning_fundamentals/#reasoning","text":"","title":"Reasoning"},{"location":"ML/reasoning_fundamentals/#intelligent-system","text":"","title":"Intelligent system"},{"location":"ML/reasoning_fundamentals/#learning-intelligent-system","text":"Knowledge is embedded in analogies or relations. Based on above premise, with enough text data, one shall be able to extract a pretty good (and intelligent) description of world and reason about it.","title":"Learning Intelligent system"},{"location":"ML/reasoning_fundamentals/#all-intelligent-arguments-can-be-inferred-from-basic-building-block-relations-and-set-of-primitive-entities","text":"Knowledge is then a set of primitive entities and corresponding relations. This need not be a fully connected graph. A complete knowlege system is one, where one can reason about any two entities in the entity space. Reasoning is then defined as 1. input -> entity_1 2. relation(entity_2) -> entity_2","title":"All intelligent arguments can be inferred from basic building block relations and set of primitive entities."},{"location":"ML/reasoning_fundamentals/#properties-of-relations","text":"They are very much like in maths R(A, B) -> {true| false} R(A) -> B R^(B) -> A Also, given A, B -> one should be able to find the relation (important for interpretability) Note: this does not mean that R^ must exist as a primitive relation. A reasoning system must be able to construct all entities from the primitive entities using convex combination. Imagine a spiders web. If primitive entities are represented at the outer edges of the web, then all the space of new (non-primitive) entities can be created as a linear combination of primitive entities.","title":"Properties of relations"},{"location":"ML/reasoning_fundamentals/#implementation-ideas","text":"|E| -> |R| number of possible arguments can then be |R|^(number of steps of reasoning). ( Connection with group theory?? ) One can argue that given the amount of relations and entities, any argument can be made with reason. Reasoning is then an optimization process to find the shortest relation between two (new or primitive) entities. This idea can also be used to determine when to stop reasoning.","title":"Implementation ideas"},{"location":"ML/reasoning_fundamentals/#role-of-context-temporal-information-and-symbols","text":"context is then used solely to map the symbols (or any input modality) into corresponding entities.","title":"Role of context (temporal information, and symbols)"},{"location":"ML/reasoning_fundamentals/#attributes-of-learning-system","text":"learning system must be able to learn primitive entities and relations. learn the mapping function learning system must be able to contruct new relations (to reduce the amount of computation) Once set of primitive relations and primitive entities are acquired, along with the mapping (symbol, context) -> entity mapping function, a learning system should be very sample efficient in acquiring new concepts.","title":"Attributes of Learning system"},{"location":"ML/reasoning_fundamentals/#learning-a-task","text":"is tantamount to forming more dense relations amount context/task dependent entities. A > B, B > C => A > C Entities in this respect are just variables. It is the job of mapping function to map symbols to these variables.","title":"Learning a task"},{"location":"ML/reasoning_fundamentals/#learning","text":"relations that most used can follow hebbian learning rule, and thus are not the first one to be overwritten. adding new relations (must take some computation), that is an analogy must be made. a new relation (think more) is a composition of premitive relations??? thus a same inference can be obtained by either more computation by applying consequtive compositional relations on entities. Or a shortcut may be formed or repeated application (hebbian learning again?)","title":"Learning"},{"location":"ML/reasoning_fundamentals/#optimization","text":"use forward prop to form new relations? can forward prop be a one step optimization process to find the analogies?","title":"optimization"},{"location":"ML/reasoning_fundamentals/#role-of-memory","text":"Can some emtities or relation be more active due to priming effect of forward processing? Fast Weights? Perhaps priming can activate all the entities and relations connected by few steps to the last used entities and relations.","title":"Role of memory?"},{"location":"ML/reasoning_fundamentals/#demo","text":"Theorem proving? Sample Efficiency Interpretabilty?","title":"demo"},{"location":"ML/reasoning_fundamentals/#also-consider-adding-optimization-techniques-like","text":"relative position for text (transformer-xl) TPR can be a good choice for representing entities and relations","title":"also consider adding optimization techniques like"},{"location":"ML/Concepts/attensor/","text":"Context dependent embedding \u00b6 Emb(symbol) -> Attensor(context, Emb(symbol)) - Attensor can be realised using Tensor Network Experiment Attensor as a substitute for attention \u00b6 Multi-head self attention Attention language modelling show that they are special cases of attensor compare in terms of speed, accuracy and speed of convergence vs num of parameters","title":"Attensor: Tensor based attention for context dependent embeddings and language modeling"},{"location":"ML/Concepts/attensor/#context-dependent-embedding","text":"Emb(symbol) -> Attensor(context, Emb(symbol)) - Attensor can be realised using Tensor Network","title":"Context dependent embedding"},{"location":"ML/Concepts/attensor/#experiment-attensor-as-a-substitute-for-attention","text":"Multi-head self attention Attention language modelling show that they are special cases of attensor compare in terms of speed, accuracy and speed of convergence vs num of parameters","title":"Experiment Attensor as a substitute for attention"},{"location":"ML/Concepts/fast_weights/","text":"I read a sunday classic References \u00b6 Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets.","title":"Fast-Weights in RNN, from first principles"},{"location":"ML/Concepts/fast_weights/#references","text":"Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets.","title":"References"},{"location":"ML/Concepts/misc/","text":"NLP benchmark \u00b6 NLP Progress website Tranformer XL \u00b6 Code and pretrained model Paper AWD-LSTMS \u00b6 This is a great blog on the underlying concept and should help implement the same in trudle. It would be also good to see how awd-lstm compare with attentive awd-lstms. Blog RNN training methods and corresponding computational complexity \u00b6 Paper Short term memory for RNNs \u00b6 This paper propose by Jimmy Ba and Hinton, proposes an old idea of using Hebbian like learning rule on 2 nd order TPR representations to augment traditional RNN to incoorporate temporary, short-term memory. Paper Fast weight memories Video by Jimmy Video by Hinton My two cents \u00b6 Can this be combined with Neural Ordinary Differential equation to determine automatically determine the length of inner loop? Can decay parameter be learning, than have constant exponentail rate decay? It will be interesting to see how large scape Ulmfit like language model trained using truncated BPTT will do. If it works, it should be faster (and real time) during inference compared to attentional variants including Transformer XL. Symbolic Neural Reasoning using TPR \u00b6 This follows on Paul Smolensky work on symbolic reasoning using TPR and gives an end-to-end learning framework using Fast-Weights update of third order TPRs. Very interesting paper, which scope to build upon further. Paper My two cents \u00b6 Authors haven't been able to integrate it to a full fledged LSTM (or unable to make it work). The idea is novel and needs further investigation. Graph Neural Networks \u00b6 Paper Paper Blog Neural and Symbolic Reasoning \u00b6 Workshop Course FOL Theorem Proving Dataset HolStep Dataset DeepMath - Deep Sequence Models for Premise Selection My two cents \u00b6 Theorem proving and theorem validation is epitomy of symbolic reasoning. It would be interesting to see how the Fast-weights and TPR variants above do on these tasks. Logic Tensor Network \u00b6 The paper doesn't as good of a justice to the theory they develop in the tutorial. They lay the foundation of first order (fuzzy) logic in tensors. The tutorial is also a good introduction to fuzzy logic. Tutorial, Code, Notebooks Model confidence recalibration \u00b6 I stumbled upon this blog post when I noticed that after training my model for longer (and consequently obtaining higher accuracy), the confidence thresholding didn't work as well. This also hijacked the highly confident beams during beam search. The post is baed on a nicely written article, link below. Paper Pytorch Code My two cents \u00b6 This has not been tried in NLP settings. I am in process of implementing it now as part of my work at True AI. I shall post the results after. Symbolic Reasoning, Inductive Logic Programming, Link Predict, Program Synthesis \u00b6 From Machine Learning to Machine Reasoning \u00b6 Old classic and discusses some plausible requirements and neurological evidence for the basis of intelligence. - Paper Differentiable Inductive Logic Programming \u00b6 Paper Logical Rule Induction and Theory Learning Using Neural Theorem Proving \u00b6 Paper Lifted Rule Injection for Relation Embeddings \u00b6 Paper Video End-to-end differential proving \u00b6 Video - Paper Programming with a Differentiable Forth Interpreter \u00b6 Paper Code Video Differentiable Programs with Neural Libraries (Terpret) \u00b6 Paper Paper 2 Video Code Differentiable Functional Programming (scala) \u00b6 Video Differentiable Functional Program Interpreters \u00b6 Paper A curate list on code induction and synthesis \u00b6 link Adventures in Neuro Symbolic Machine Learning \u00b6 Slides Tensor Product Programming Language \u00b6 Paper Introduction to Tensor Decompositions and their Applications in Machine Learning Tensor Decomposition Weight Decay, BatchNorm and connection with learning rate \u00b6 Blog","title":"Paper Collection"},{"location":"ML/Concepts/misc/#nlp-benchmark","text":"NLP Progress website","title":"NLP benchmark"},{"location":"ML/Concepts/misc/#tranformer-xl","text":"Code and pretrained model Paper","title":"Tranformer XL"},{"location":"ML/Concepts/misc/#awd-lstms","text":"This is a great blog on the underlying concept and should help implement the same in trudle. It would be also good to see how awd-lstm compare with attentive awd-lstms. Blog","title":"AWD-LSTMS"},{"location":"ML/Concepts/misc/#rnn-training-methods-and-corresponding-computational-complexity","text":"Paper","title":"RNN training methods and corresponding computational complexity"},{"location":"ML/Concepts/misc/#short-term-memory-for-rnns","text":"This paper propose by Jimmy Ba and Hinton, proposes an old idea of using Hebbian like learning rule on 2 nd order TPR representations to augment traditional RNN to incoorporate temporary, short-term memory. Paper Fast weight memories Video by Jimmy Video by Hinton","title":"Short term memory for RNNs"},{"location":"ML/Concepts/misc/#my-two-cents","text":"Can this be combined with Neural Ordinary Differential equation to determine automatically determine the length of inner loop? Can decay parameter be learning, than have constant exponentail rate decay? It will be interesting to see how large scape Ulmfit like language model trained using truncated BPTT will do. If it works, it should be faster (and real time) during inference compared to attentional variants including Transformer XL.","title":"My two cents"},{"location":"ML/Concepts/misc/#symbolic-neural-reasoning-using-tpr","text":"This follows on Paul Smolensky work on symbolic reasoning using TPR and gives an end-to-end learning framework using Fast-Weights update of third order TPRs. Very interesting paper, which scope to build upon further. Paper","title":"Symbolic Neural Reasoning using TPR"},{"location":"ML/Concepts/misc/#my-two-cents_1","text":"Authors haven't been able to integrate it to a full fledged LSTM (or unable to make it work). The idea is novel and needs further investigation.","title":"My two cents"},{"location":"ML/Concepts/misc/#graph-neural-networks","text":"Paper Paper Blog","title":"Graph Neural Networks"},{"location":"ML/Concepts/misc/#neural-and-symbolic-reasoning","text":"Workshop Course FOL Theorem Proving Dataset HolStep Dataset DeepMath - Deep Sequence Models for Premise Selection","title":"Neural and Symbolic Reasoning"},{"location":"ML/Concepts/misc/#my-two-cents_2","text":"Theorem proving and theorem validation is epitomy of symbolic reasoning. It would be interesting to see how the Fast-weights and TPR variants above do on these tasks.","title":"My two cents"},{"location":"ML/Concepts/misc/#logic-tensor-network","text":"The paper doesn't as good of a justice to the theory they develop in the tutorial. They lay the foundation of first order (fuzzy) logic in tensors. The tutorial is also a good introduction to fuzzy logic. Tutorial, Code, Notebooks","title":"Logic Tensor Network"},{"location":"ML/Concepts/misc/#model-confidence-recalibration","text":"I stumbled upon this blog post when I noticed that after training my model for longer (and consequently obtaining higher accuracy), the confidence thresholding didn't work as well. This also hijacked the highly confident beams during beam search. The post is baed on a nicely written article, link below. Paper Pytorch Code","title":"Model confidence recalibration"},{"location":"ML/Concepts/misc/#my-two-cents_3","text":"This has not been tried in NLP settings. I am in process of implementing it now as part of my work at True AI. I shall post the results after.","title":"My two cents"},{"location":"ML/Concepts/misc/#symbolic-reasoning-inductive-logic-programming-link-predict-program-synthesis","text":"","title":"Symbolic Reasoning, Inductive Logic Programming, Link Predict, Program Synthesis"},{"location":"ML/Concepts/misc/#from-machine-learning-to-machine-reasoning","text":"Old classic and discusses some plausible requirements and neurological evidence for the basis of intelligence. - Paper","title":"From Machine Learning to Machine Reasoning"},{"location":"ML/Concepts/misc/#differentiable-inductive-logic-programming","text":"Paper","title":"Differentiable Inductive Logic Programming"},{"location":"ML/Concepts/misc/#logical-rule-induction-and-theory-learning-using-neural-theorem-proving","text":"Paper","title":"Logical Rule Induction and Theory Learning Using Neural Theorem Proving"},{"location":"ML/Concepts/misc/#lifted-rule-injection-for-relation-embeddings","text":"Paper Video","title":"Lifted Rule Injection for Relation Embeddings"},{"location":"ML/Concepts/misc/#end-to-end-differential-proving","text":"Video - Paper","title":"End-to-end differential proving"},{"location":"ML/Concepts/misc/#programming-with-a-differentiable-forth-interpreter","text":"Paper Code Video","title":"Programming with a Differentiable Forth Interpreter"},{"location":"ML/Concepts/misc/#differentiable-programs-with-neural-libraries-terpret","text":"Paper Paper 2 Video Code","title":"Differentiable Programs with Neural Libraries (Terpret)"},{"location":"ML/Concepts/misc/#differentiable-functional-programming-scala","text":"Video","title":"Differentiable Functional Programming (scala)"},{"location":"ML/Concepts/misc/#differentiable-functional-program-interpreters","text":"Paper","title":"Differentiable Functional Program Interpreters"},{"location":"ML/Concepts/misc/#a-curate-list-on-code-induction-and-synthesis","text":"link","title":"A curate list on code induction and synthesis"},{"location":"ML/Concepts/misc/#adventures-in-neuro-symbolic-machine-learning","text":"Slides","title":"Adventures in Neuro Symbolic Machine Learning"},{"location":"ML/Concepts/misc/#tensor-product-programming-language","text":"Paper Introduction to Tensor Decompositions and their Applications in Machine Learning Tensor Decomposition","title":"Tensor Product Programming Language"},{"location":"ML/Concepts/misc/#weight-decay-batchnorm-and-connection-with-learning-rate","text":"Blog","title":"Weight Decay, BatchNorm and connection with learning rate"},{"location":"ML/Concepts/stochastic_gradient_estimation/","text":"Stochastic Gradient Estimation \u00b6 Problem statement \u00b6 We are given \"well behaved\" 1 distribution q_{\\phi}(z),\\ z \\in \\mathbb{R},\\ \\phi \\in \\mathbb{R} q_{\\phi}(z),\\ z \\in \\mathbb{R},\\ \\phi \\in \\mathbb{R} . We are also given cost function f(z) \\in \\mathbb{R} f(z) \\in \\mathbb{R} and a loss function \\mathcal{L}(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[f(z)] \\mathcal{L}(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[f(z)] . We want to calculate unbiased 2 stochastic estimator for \\mathcal{L}'(\\phi) \\mathcal{L}'(\\phi) \\mathcal{L}'(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[g(z, \\phi)] \\mathcal{L}'(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[g(z, \\phi)] As such, if we can calculate the gradient g(z, \\phi) g(z, \\phi) then we can compute Monte Carlo estimate for practical applications. \\mathcal{L}'(\\phi) \\approx \\frac{1}{M}\\sum_{i=1}^M g(z_i, \\phi),\\ \\ z_i \\sim q_{\\phi}(z) \\mathcal{L}'(\\phi) \\approx \\frac{1}{M}\\sum_{i=1}^M g(z_i, \\phi),\\ \\ z_i \\sim q_{\\phi}(z) Reference \u00b6 Michael Figurnov (DeepMind) in DeepBayes2018 Differentiable density and no inner regions of zero density. \u21a9 Unbiased just means that if expecation is calculated for infinite samples, then we converge to the true expectation without any offset. \u21a9","title":"Stochastic Gradient Estimation"},{"location":"ML/Concepts/stochastic_gradient_estimation/#stochastic-gradient-estimation","text":"","title":"Stochastic Gradient Estimation"},{"location":"ML/Concepts/stochastic_gradient_estimation/#problem-statement","text":"We are given \"well behaved\" 1 distribution q_{\\phi}(z),\\ z \\in \\mathbb{R},\\ \\phi \\in \\mathbb{R} q_{\\phi}(z),\\ z \\in \\mathbb{R},\\ \\phi \\in \\mathbb{R} . We are also given cost function f(z) \\in \\mathbb{R} f(z) \\in \\mathbb{R} and a loss function \\mathcal{L}(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[f(z)] \\mathcal{L}(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[f(z)] . We want to calculate unbiased 2 stochastic estimator for \\mathcal{L}'(\\phi) \\mathcal{L}'(\\phi) \\mathcal{L}'(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[g(z, \\phi)] \\mathcal{L}'(\\phi) = \\mathbb{E}_{q_{\\phi}(z)}[g(z, \\phi)] As such, if we can calculate the gradient g(z, \\phi) g(z, \\phi) then we can compute Monte Carlo estimate for practical applications. \\mathcal{L}'(\\phi) \\approx \\frac{1}{M}\\sum_{i=1}^M g(z_i, \\phi),\\ \\ z_i \\sim q_{\\phi}(z) \\mathcal{L}'(\\phi) \\approx \\frac{1}{M}\\sum_{i=1}^M g(z_i, \\phi),\\ \\ z_i \\sim q_{\\phi}(z)","title":"Problem statement"},{"location":"ML/Concepts/stochastic_gradient_estimation/#reference","text":"Michael Figurnov (DeepMind) in DeepBayes2018 Differentiable density and no inner regions of zero density. \u21a9 Unbiased just means that if expecation is calculated for infinite samples, then we converge to the true expectation without any offset. \u21a9","title":"Reference"},{"location":"ML/PaperSummaries/symbolic_dl/","text":"Link to article I must admit that I discovered this article trying to build my original ideas, and I was pleasantly surprised on how it overlapped 80%-90% on what I had come to conclude even before reading this. It is reassuring that to know that people who are far more qualified, and who have spent way more time than I have on this topic have come to similar conclusion (albeit with more references). Self adulation aside, lets get to business. Deep Learning \u00b6 Shortcoming in Deep Learning \u00b6 They categorize (with examples) which are fairly common and intuitive with current Deep Learning method. Data Inefficiency : Need no less than a simulator for Deep RL and millions of samples in old-fashioned DL. Poor Generalization : In ability to transfer or reuse learning to a (even slightly) different domain. Lack of Interpretability : Black box phenomenon. Strengths of DL \u00b6 Data driven automatic representation (and compute) learning : Throw in huge amount of data, and a suitable optimizer and wait. Symbolic AI \u00b6 Most predominant until 1980s, in symbolic AI, representations are typically propositional in character, and assert that certain relations hold between certain objects, while each reasoning step computes a further set of relations that follow from those already established, according to a formally specified set of inference rules. As reminder (and mathematically speaking), a proposition is a statement that evaluates to either True or False. Predicate is a function or mapping that takes variables produce propositional output. A relation is a predicate of two variables. Shortcoming of Symbolic AI \u00b6 Hand crafted representation : It is not clear how the representations can be learned from data and have traditionally been hand crafted to be used. This is known as symbol grounding problem . The contrast with DL is obvious. Strengths of Symbolic AI \u00b6 Strengths typically align with shortcomings of DL. Data efficiency : thanks to their declarative nature, symbolic representations lend themselves to re-use in multiple tasks, which promotes data efficiency. There is mostly no training needed, as the rules are often handcrafted. Cross-domain generalization : Symbolic representations tend to be high-level and abstract, which facilitates generalisation. As long as symbols are mapped to correct variables, and problem is correctly formulation in Symbolic AI language. Interpretable : Because of their language-like, propositional character, symbolic representations are amenable to human understanding. The predetermined rules of inference can be examined by humans. WIP, more to follow. Compositionality \u00b6 Show that this can be done using Tensors Relational Representation \u00b6 Can be done using Tensors It is easy to show that each layer of feed-forward network is than one function.","title":"Reconciling deep learning with symbolic artificial intelligence"},{"location":"ML/PaperSummaries/symbolic_dl/#deep-learning","text":"","title":"Deep Learning"},{"location":"ML/PaperSummaries/symbolic_dl/#shortcoming-in-deep-learning","text":"They categorize (with examples) which are fairly common and intuitive with current Deep Learning method. Data Inefficiency : Need no less than a simulator for Deep RL and millions of samples in old-fashioned DL. Poor Generalization : In ability to transfer or reuse learning to a (even slightly) different domain. Lack of Interpretability : Black box phenomenon.","title":"Shortcoming in Deep Learning"},{"location":"ML/PaperSummaries/symbolic_dl/#strengths-of-dl","text":"Data driven automatic representation (and compute) learning : Throw in huge amount of data, and a suitable optimizer and wait.","title":"Strengths of DL"},{"location":"ML/PaperSummaries/symbolic_dl/#symbolic-ai","text":"Most predominant until 1980s, in symbolic AI, representations are typically propositional in character, and assert that certain relations hold between certain objects, while each reasoning step computes a further set of relations that follow from those already established, according to a formally specified set of inference rules. As reminder (and mathematically speaking), a proposition is a statement that evaluates to either True or False. Predicate is a function or mapping that takes variables produce propositional output. A relation is a predicate of two variables.","title":"Symbolic AI"},{"location":"ML/PaperSummaries/symbolic_dl/#shortcoming-of-symbolic-ai","text":"Hand crafted representation : It is not clear how the representations can be learned from data and have traditionally been hand crafted to be used. This is known as symbol grounding problem . The contrast with DL is obvious.","title":"Shortcoming of Symbolic AI"},{"location":"ML/PaperSummaries/symbolic_dl/#strengths-of-symbolic-ai","text":"Strengths typically align with shortcomings of DL. Data efficiency : thanks to their declarative nature, symbolic representations lend themselves to re-use in multiple tasks, which promotes data efficiency. There is mostly no training needed, as the rules are often handcrafted. Cross-domain generalization : Symbolic representations tend to be high-level and abstract, which facilitates generalisation. As long as symbols are mapped to correct variables, and problem is correctly formulation in Symbolic AI language. Interpretable : Because of their language-like, propositional character, symbolic representations are amenable to human understanding. The predetermined rules of inference can be examined by humans. WIP, more to follow.","title":"Strengths of Symbolic AI"},{"location":"ML/PaperSummaries/symbolic_dl/#compositionality","text":"Show that this can be done using Tensors","title":"Compositionality"},{"location":"ML/PaperSummaries/symbolic_dl/#relational-representation","text":"Can be done using Tensors It is easy to show that each layer of feed-forward network is than one function.","title":"Relational Representation"},{"location":"Maths/fuzzy_logic/","text":"Logic (from the Greek \"logos\", which has a variety of meanings including word, thought, idea, argument, account, reason or principle) is the study of reasoning, or the study of the principles and criteria of valid inference and demonstration. It attempts to distinguish good reasoning from bad reasoning. Conventional Logic typically deals with binary truth values {True/False}. However, real world (and human feelings for certain) seldom works in binary. As humans, we constantly make choices on vague or imprecise judgements. For example, one may not know for sure if it is going to rain (unless of course you are living in London, when not a day goes by without raining) and yet she may decide to carry an umbrella, there by making binary decision based on non-binary reasoning. For computers to be able to reason in real world and to solve real world problems, they must be equipped to make sense of such fuzzy data which is neither completely true, nor completely false. That is where Fuzzy Logic comes to the rescue. It is a branch of many-valued symbolic logic which has comparative notion of truth, syntax, semantics, axiomatization, truth-preserving, completeness, etc. Broadly speaking, Fuzzy logic serves mainly as apparatus for fuzzy control, analysis of vagueness in natural language and several other application domains. It is one of the techniques of soft-computing, i.e. computational methods tolerant to suboptimality and impreciseness (vagueness) and giving quick, simple and sufficiently good solutions. In this blog post, I will summarise Fuzzy propositional logic. This post is not intended to be a rigorous mathematical treatment of the subject but rather a cheat-sheet that can be consulted to revise and reference important concepts. Crisp and Fuzzy Sets \u00b6 In classical mathematics, one deals with collection of objects called sets . It is usually convenient to fix some universe U U in which every set is assumed to be included 1 . Then a set A A can be thought of as a function on U U which takes a values of 1 1 for objects that belong to A A , 0 0 otherwise. Such a function is called the characteristic function of A A , \\chi_A(.) \\chi_A(.) : \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} Obviously, this implies that there exist a bijective mapping between characteristic function and maps. In fact, this is how sets are defined in ZFC Set Theory. Crisp Set Let X X be the set of all real numbers between 0 and 10 and let A = [5, 9] A = [5, 9] be the subset of X X of real numbers between 5 and 9. This results in the following figure: Fuzzy sets generalise this definition, allowing elements to belong to a given set with a certain degree. Instead of considering characteristic functions with value in {0, 1} {0, 1} , we consider now functions valued in the interval [0, 1] [0, 1] . A fuzzy subset F F of a set U U is a function \\mu_F(\u00b7) \\mu_F(\u00b7) assigning to every element x \\in U x \\in U the degree of membership of x x to F F : \\mu_F : U \\rightarrow [0, 1] \\mu_F : U \\rightarrow [0, 1] Fuzzy Set Let, as above, U be the set of real numbers between 1 and 10. A description of the fuzzy set of real numbers close to 7 could be given by the following figure: Operations between crisp sets \u00b6 In classical set theory there are some basic operations defined over sets. Let U U be a set and 2^U 2^U (also known as power set) be the set of all subsets of U. Since there exists, a bijection between sets and the corresponding characteristic function, one could equivalently say that a power set, 2^U 2^U is a set defined by a characteristic function \\chi_{2^U}: U \\rightarrow \\{0, 1\\} \\chi_{2^U}: U \\rightarrow \\{0, 1\\} . The operation of union, intersection and complement are defined in the following ways: Operation Crisp Set Characteristic Function Union A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) Intersection A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) Compliment \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x) \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x) Operations between fuzzy sets \u00b6 While concept of binary membership ( \\in \\in ) does not exist in fuzzy sets, we can draw parallels to the characteristic function for crisp set and apply that to membership functions of fuzzy set analogously to derive set operations. Operation Fuzzy Set Membership Function Union \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) Intersection \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) Compliment \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) Fuzzy Set Operations Let A A and Bs Bs be fuzzy subsets of U U given by membership functions \\mu_A \\mu_A and \\mu_B \\mu_B : Union Intersection Compliment \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) Fuzzy propositional logic \u00b6 In classical propositional logic, a propositional language consists of only propositions, i.e., statements that can be assigned either True or False. Statements of the form \"It is raining.\", \"Today is Sunday.\", \"2+2=5\" are all examples of valid propositions. \"Open the door\", \"Ecstatic feeling\", \"Run!\", \"How many planets are there in solar system?\" cannot be assigned simple binary truth values and are thus not propositions. It is convenient to define unary and binary operations on propositions. A unary operator takes one proposition to create a new proposition. Since there are two truth values, there are exactly four ( 2^2 2^2 ) unary operators, there are Identity, Compliment, Tautology and Contradiction. X X Identity (X) (X) Compliment \\bar{X} \\bar{X} Tautology \\top(X) \\top(X) Contradiction \\bot(X) \\bot(X) T T F T F F F T T F A binary operator (often called a connective) takes two proposition to create a new proposition. Consequently, it is possible to define 16 ( 2^4 2^4 ) binary connectives. Most prominent of those are Conjunction, Disjunction, Implication, Equivalence, and Xor. It is easy to show that all 16 connective equivalent can be obtained by unique application of these binary connectives 2 . X Y X \\wedge Y X \\wedge Y X \\lor Y X \\lor Y X \\Rightarrow Y X \\Rightarrow Y X \\Leftrightarrow Y X \\Leftrightarrow Y X \\oplus Y X \\oplus Y T T T T T T F T F F T F F T F T F T T F T F F F F T T F We can generalize the classical propositional logic to incorporate fuzziness, such that proposition values are in [0, 1] [0, 1] and that if x > y x > y , then x x is more true than y y . Fuzzy Conjunction: Triangular-norm (T-norm) \u00b6 The conjunction connective in fuzzy logic is formalized by a binary operation on truth values, called t-norm , which satisfy a minimal set of properties to capture the intuitive meaning of conjunction. Definition T-norm A t-norm is a binary operation \\otimes:[0, 1]^2 \\rightarrow [0, 1] \\otimes:[0, 1]^2 \\rightarrow [0, 1] satisfying the following conditions: Commutativity: x \\otimes y = y \\otimes x x \\otimes y = y \\otimes x Associativity: x \\otimes (y \\otimes z) = (y \\otimes x) \\otimes z x \\otimes (y \\otimes z) = (y \\otimes x) \\otimes z Non-decreasing: x \\leq y \\Rightarrow z \\otimes x \\leq z \\otimes y x \\leq y \\Rightarrow z \\otimes x \\leq z \\otimes y Zero and One: 0 \\otimes x = 0 0 \\otimes x = 0 and 1 \\otimes x = x 1 \\otimes x = x A t-norm \\otimes \\otimes is continuous if the function \\otimes:[0, 1]^2 \\rightarrow [0, 1] \\otimes:[0, 1]^2 \\rightarrow [0, 1] is a continuous function in the usual sense. Examples \u00b6 Name Definition Visualization Lukasiewicz t-norm x \\otimes y = max(0, x + y \u2212 1) x \\otimes y = max(0, x + y \u2212 1) G\u0308odel t-norm x \\otimes y = min(x, y) x \\otimes y = min(x, y) Product t-norm x \\otimes y = x \\cdot y x \\otimes y = x \\cdot y Fuzzy Disjunction: T-Conorm (or S-norm) \u00b6 The conjunction connective in fuzzy logic is formalized by a binary operation on truth values, called t-conorm , which satisfy a minimal set of properties to capture the intuitive meaning of disjunction. The name t-conorm is comes from the fact that disjunction is a dual operation to conjunction. Definition T-conorm (also called S-norm) A t-conorm is a binary operation \\oplus:[0, 1]^2 \\rightarrow [0, 1] \\oplus:[0, 1]^2 \\rightarrow [0, 1] satisfying the following conditions: Commutativity: x \\oplus y = y \\oplus x x \\oplus y = y \\oplus x Associativity: x \\oplus (y \\oplus z) = (y \\oplus x) \\oplus z x \\oplus (y \\oplus z) = (y \\oplus x) \\oplus z Non-decreasing: x \\leq y \\Rightarrow z \\oplus x \\leq z \\oplus y x \\leq y \\Rightarrow z \\oplus x \\leq z \\oplus y Zero and One: 0 \\oplus x = x 0 \\oplus x = x and 1 \\oplus x = 1 1 \\oplus x = 1 Given a t-norm \\otimes \\otimes , a corresponding t-conorm can be derived as x \\oplus y = 1 - ( 1 - x ) \\otimes ( 1 - y) x \\oplus y = 1 - ( 1 - x ) \\otimes ( 1 - y) Examples \u00b6 Name Definition Visualization Lukasiewicz t-conorm x \\oplus y = min(1, x + y) x \\oplus y = min(1, x + y) G\u0308odel t-conorm x \\oplus y = max(x, y) x \\oplus y = max(x, y) Product t-conotm x \\oplus y = x + y - x \\cdot y x \\oplus y = x + y - x \\cdot y Fuzzy Implication: Residuum \u00b6 Intuitively the more x \\Rightarrow y x \\Rightarrow y is true, the less additional information is carried by y y w.r.t., x x . Thus, for t-norm \\otimes \\otimes , following property should hold for residuum \\Rightarrow \\Rightarrow : z \\otimes x \\quad if \\; and \\; only \\; if \\quad z \\leq ( x \\Rightarrow y) z \\otimes x \\quad if \\; and \\; only \\; if \\quad z \\leq ( x \\Rightarrow y) Semantics of residuum are therefore defines as the maximum truth value to be \"added\" to x x to obtain y y . Definition Residuum The residuum of the t-norm \\otimes \\otimes is a binary operation \\Rightarrow:[0, 1]^2 \\rightarrow [0, 1] \\Rightarrow:[0, 1]^2 \\rightarrow [0, 1] is defined as (x \\Rightarrow y) = max(\\{ z \\mid x \\otimes z <= y\\}) (x \\Rightarrow y) = max(\\{ z \\mid x \\otimes z <= y\\}) Properties of Residua: If x \\leq y x \\leq y then (x \\Rightarrow y) = 1 (x \\Rightarrow y) = 1 (1 \\Rightarrow x) = x (1 \\Rightarrow x) = x (x \\Rightarrow 1) = 1 (x \\Rightarrow 1) = 1 If x \\leq y x \\leq y then x = y \\otimes (y \\Rightarrow x) x = y \\otimes (y \\Rightarrow x) Examples \u00b6 If x \\leq y x \\leq y , then (x \\Rightarrow y) = 1 (x \\Rightarrow y) = 1 , however, when x > y x > y , then Name Definition Visualization Lukasiewicz Residuum (x \\Rightarrow y) = 1 - x + y (x \\Rightarrow y) = 1 - x + y G\u0308odel Residuum (x \\Rightarrow y) = y (x \\Rightarrow y) = y Product Residuum (x \\Rightarrow y) = y/x (x \\Rightarrow y) = y/x Fuzzy Negation: Precompliment \u00b6 In classical propositional logic, negation can be thought as implication of contradiction, i.e., \\bar{X} \\Leftrightarrow (X \\Rightarrow \\bot) \\bar{X} \\Leftrightarrow (X \\Rightarrow \\bot) can shown in table below X X \\bot \\bot (X \\Rightarrow \\bot) (X \\Rightarrow \\bot) \\bar{X} \\bar{X} T F F F F F T T Negation can be defined analogously for fuzzy propositional logic. Definition Precompliment For every residual operator $ \\Rightarrow $ (and therefore for every t-norm), the precomplement operator denoted by (\u2212) (\u2212) , is defined as: (-)x = (x \\Rightarrow 0) (-)x = (x \\Rightarrow 0) Examples \u00b6 Name Definition Lukasiewicz precompliment (-)x = 1 - x (-)x = 1 - x G\u0308odel precompliment (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} Product precompliment (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} This concludes the post on fuzzy propositional logic. The next step to study fuzzy first order logic or fuzzy predicate logic involving quantors, namely for all ( \\forall \\forall ) and there exists ( \\exists \\exists ). References \u00b6 Neural-Symbolic Learning and Reasoning with Constraints Tutorial at IEEE IJCNN 2018 T-norm, Wikipedia Refer to ZFC Set theory and famous Russel's paradox for more rigorous arguments. \u21a9 Infact, it can be shown that only NAND or only NOR operation is complete and all other propositional transformation circuits can be created from NAND or NOR alone. \u21a9","title":"Fuzzy Propositional Logic"},{"location":"Maths/fuzzy_logic/#crisp-and-fuzzy-sets","text":"In classical mathematics, one deals with collection of objects called sets . It is usually convenient to fix some universe U U in which every set is assumed to be included 1 . Then a set A A can be thought of as a function on U U which takes a values of 1 1 for objects that belong to A A , 0 0 otherwise. Such a function is called the characteristic function of A A , \\chi_A(.) \\chi_A(.) : \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} Obviously, this implies that there exist a bijective mapping between characteristic function and maps. In fact, this is how sets are defined in ZFC Set Theory. Crisp Set Let X X be the set of all real numbers between 0 and 10 and let A = [5, 9] A = [5, 9] be the subset of X X of real numbers between 5 and 9. This results in the following figure: Fuzzy sets generalise this definition, allowing elements to belong to a given set with a certain degree. Instead of considering characteristic functions with value in {0, 1} {0, 1} , we consider now functions valued in the interval [0, 1] [0, 1] . A fuzzy subset F F of a set U U is a function \\mu_F(\u00b7) \\mu_F(\u00b7) assigning to every element x \\in U x \\in U the degree of membership of x x to F F : \\mu_F : U \\rightarrow [0, 1] \\mu_F : U \\rightarrow [0, 1] Fuzzy Set Let, as above, U be the set of real numbers between 1 and 10. A description of the fuzzy set of real numbers close to 7 could be given by the following figure:","title":"Crisp and Fuzzy Sets"},{"location":"Maths/fuzzy_logic/#operations-between-crisp-sets","text":"In classical set theory there are some basic operations defined over sets. Let U U be a set and 2^U 2^U (also known as power set) be the set of all subsets of U. Since there exists, a bijection between sets and the corresponding characteristic function, one could equivalently say that a power set, 2^U 2^U is a set defined by a characteristic function \\chi_{2^U}: U \\rightarrow \\{0, 1\\} \\chi_{2^U}: U \\rightarrow \\{0, 1\\} . The operation of union, intersection and complement are defined in the following ways: Operation Crisp Set Characteristic Function Union A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) Intersection A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) Compliment \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x) \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x)","title":"Operations between crisp sets"},{"location":"Maths/fuzzy_logic/#operations-between-fuzzy-sets","text":"While concept of binary membership ( \\in \\in ) does not exist in fuzzy sets, we can draw parallels to the characteristic function for crisp set and apply that to membership functions of fuzzy set analogously to derive set operations. Operation Fuzzy Set Membership Function Union \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) Intersection \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) Compliment \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) Fuzzy Set Operations Let A A and Bs Bs be fuzzy subsets of U U given by membership functions \\mu_A \\mu_A and \\mu_B \\mu_B : Union Intersection Compliment \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x)","title":"Operations between fuzzy sets"},{"location":"Maths/fuzzy_logic/#fuzzy-propositional-logic","text":"In classical propositional logic, a propositional language consists of only propositions, i.e., statements that can be assigned either True or False. Statements of the form \"It is raining.\", \"Today is Sunday.\", \"2+2=5\" are all examples of valid propositions. \"Open the door\", \"Ecstatic feeling\", \"Run!\", \"How many planets are there in solar system?\" cannot be assigned simple binary truth values and are thus not propositions. It is convenient to define unary and binary operations on propositions. A unary operator takes one proposition to create a new proposition. Since there are two truth values, there are exactly four ( 2^2 2^2 ) unary operators, there are Identity, Compliment, Tautology and Contradiction. X X Identity (X) (X) Compliment \\bar{X} \\bar{X} Tautology \\top(X) \\top(X) Contradiction \\bot(X) \\bot(X) T T F T F F F T T F A binary operator (often called a connective) takes two proposition to create a new proposition. Consequently, it is possible to define 16 ( 2^4 2^4 ) binary connectives. Most prominent of those are Conjunction, Disjunction, Implication, Equivalence, and Xor. It is easy to show that all 16 connective equivalent can be obtained by unique application of these binary connectives 2 . X Y X \\wedge Y X \\wedge Y X \\lor Y X \\lor Y X \\Rightarrow Y X \\Rightarrow Y X \\Leftrightarrow Y X \\Leftrightarrow Y X \\oplus Y X \\oplus Y T T T T T T F T F F T F F T F T F T T F T F F F F T T F We can generalize the classical propositional logic to incorporate fuzziness, such that proposition values are in [0, 1] [0, 1] and that if x > y x > y , then x x is more true than y y .","title":"Fuzzy propositional logic"},{"location":"Maths/fuzzy_logic/#fuzzy-conjunction-triangular-norm-t-norm","text":"The conjunction connective in fuzzy logic is formalized by a binary operation on truth values, called t-norm , which satisfy a minimal set of properties to capture the intuitive meaning of conjunction. Definition T-norm A t-norm is a binary operation \\otimes:[0, 1]^2 \\rightarrow [0, 1] \\otimes:[0, 1]^2 \\rightarrow [0, 1] satisfying the following conditions: Commutativity: x \\otimes y = y \\otimes x x \\otimes y = y \\otimes x Associativity: x \\otimes (y \\otimes z) = (y \\otimes x) \\otimes z x \\otimes (y \\otimes z) = (y \\otimes x) \\otimes z Non-decreasing: x \\leq y \\Rightarrow z \\otimes x \\leq z \\otimes y x \\leq y \\Rightarrow z \\otimes x \\leq z \\otimes y Zero and One: 0 \\otimes x = 0 0 \\otimes x = 0 and 1 \\otimes x = x 1 \\otimes x = x A t-norm \\otimes \\otimes is continuous if the function \\otimes:[0, 1]^2 \\rightarrow [0, 1] \\otimes:[0, 1]^2 \\rightarrow [0, 1] is a continuous function in the usual sense.","title":"Fuzzy Conjunction: Triangular-norm (T-norm)"},{"location":"Maths/fuzzy_logic/#examples","text":"Name Definition Visualization Lukasiewicz t-norm x \\otimes y = max(0, x + y \u2212 1) x \\otimes y = max(0, x + y \u2212 1) G\u0308odel t-norm x \\otimes y = min(x, y) x \\otimes y = min(x, y) Product t-norm x \\otimes y = x \\cdot y x \\otimes y = x \\cdot y","title":"Examples"},{"location":"Maths/fuzzy_logic/#fuzzy-disjunction-t-conorm-or-s-norm","text":"The conjunction connective in fuzzy logic is formalized by a binary operation on truth values, called t-conorm , which satisfy a minimal set of properties to capture the intuitive meaning of disjunction. The name t-conorm is comes from the fact that disjunction is a dual operation to conjunction. Definition T-conorm (also called S-norm) A t-conorm is a binary operation \\oplus:[0, 1]^2 \\rightarrow [0, 1] \\oplus:[0, 1]^2 \\rightarrow [0, 1] satisfying the following conditions: Commutativity: x \\oplus y = y \\oplus x x \\oplus y = y \\oplus x Associativity: x \\oplus (y \\oplus z) = (y \\oplus x) \\oplus z x \\oplus (y \\oplus z) = (y \\oplus x) \\oplus z Non-decreasing: x \\leq y \\Rightarrow z \\oplus x \\leq z \\oplus y x \\leq y \\Rightarrow z \\oplus x \\leq z \\oplus y Zero and One: 0 \\oplus x = x 0 \\oplus x = x and 1 \\oplus x = 1 1 \\oplus x = 1 Given a t-norm \\otimes \\otimes , a corresponding t-conorm can be derived as x \\oplus y = 1 - ( 1 - x ) \\otimes ( 1 - y) x \\oplus y = 1 - ( 1 - x ) \\otimes ( 1 - y)","title":"Fuzzy Disjunction: T-Conorm (or S-norm)"},{"location":"Maths/fuzzy_logic/#examples_1","text":"Name Definition Visualization Lukasiewicz t-conorm x \\oplus y = min(1, x + y) x \\oplus y = min(1, x + y) G\u0308odel t-conorm x \\oplus y = max(x, y) x \\oplus y = max(x, y) Product t-conotm x \\oplus y = x + y - x \\cdot y x \\oplus y = x + y - x \\cdot y","title":"Examples"},{"location":"Maths/fuzzy_logic/#fuzzy-implication-residuum","text":"Intuitively the more x \\Rightarrow y x \\Rightarrow y is true, the less additional information is carried by y y w.r.t., x x . Thus, for t-norm \\otimes \\otimes , following property should hold for residuum \\Rightarrow \\Rightarrow : z \\otimes x \\quad if \\; and \\; only \\; if \\quad z \\leq ( x \\Rightarrow y) z \\otimes x \\quad if \\; and \\; only \\; if \\quad z \\leq ( x \\Rightarrow y) Semantics of residuum are therefore defines as the maximum truth value to be \"added\" to x x to obtain y y . Definition Residuum The residuum of the t-norm \\otimes \\otimes is a binary operation \\Rightarrow:[0, 1]^2 \\rightarrow [0, 1] \\Rightarrow:[0, 1]^2 \\rightarrow [0, 1] is defined as (x \\Rightarrow y) = max(\\{ z \\mid x \\otimes z <= y\\}) (x \\Rightarrow y) = max(\\{ z \\mid x \\otimes z <= y\\}) Properties of Residua: If x \\leq y x \\leq y then (x \\Rightarrow y) = 1 (x \\Rightarrow y) = 1 (1 \\Rightarrow x) = x (1 \\Rightarrow x) = x (x \\Rightarrow 1) = 1 (x \\Rightarrow 1) = 1 If x \\leq y x \\leq y then x = y \\otimes (y \\Rightarrow x) x = y \\otimes (y \\Rightarrow x)","title":"Fuzzy Implication: Residuum"},{"location":"Maths/fuzzy_logic/#examples_2","text":"If x \\leq y x \\leq y , then (x \\Rightarrow y) = 1 (x \\Rightarrow y) = 1 , however, when x > y x > y , then Name Definition Visualization Lukasiewicz Residuum (x \\Rightarrow y) = 1 - x + y (x \\Rightarrow y) = 1 - x + y G\u0308odel Residuum (x \\Rightarrow y) = y (x \\Rightarrow y) = y Product Residuum (x \\Rightarrow y) = y/x (x \\Rightarrow y) = y/x","title":"Examples"},{"location":"Maths/fuzzy_logic/#fuzzy-negation-precompliment","text":"In classical propositional logic, negation can be thought as implication of contradiction, i.e., \\bar{X} \\Leftrightarrow (X \\Rightarrow \\bot) \\bar{X} \\Leftrightarrow (X \\Rightarrow \\bot) can shown in table below X X \\bot \\bot (X \\Rightarrow \\bot) (X \\Rightarrow \\bot) \\bar{X} \\bar{X} T F F F F F T T Negation can be defined analogously for fuzzy propositional logic. Definition Precompliment For every residual operator $ \\Rightarrow $ (and therefore for every t-norm), the precomplement operator denoted by (\u2212) (\u2212) , is defined as: (-)x = (x \\Rightarrow 0) (-)x = (x \\Rightarrow 0)","title":"Fuzzy Negation: Precompliment"},{"location":"Maths/fuzzy_logic/#examples_3","text":"Name Definition Lukasiewicz precompliment (-)x = 1 - x (-)x = 1 - x G\u0308odel precompliment (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} Product precompliment (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} (-)x = \\begin{cases} 1 & x = 0 \\\\ 0 & otherwise \\end{cases} This concludes the post on fuzzy propositional logic. The next step to study fuzzy first order logic or fuzzy predicate logic involving quantors, namely for all ( \\forall \\forall ) and there exists ( \\exists \\exists ).","title":"Examples"},{"location":"Maths/fuzzy_logic/#references","text":"Neural-Symbolic Learning and Reasoning with Constraints Tutorial at IEEE IJCNN 2018 T-norm, Wikipedia Refer to ZFC Set theory and famous Russel's paradox for more rigorous arguments. \u21a9 Infact, it can be shown that only NAND or only NOR operation is complete and all other propositional transformation circuits can be created from NAND or NOR alone. \u21a9","title":"References"},{"location":"Maths/variational_calculus/","text":"Calculus of variations: Euler-Lagrange Equation \u00b6 Did you know that you could derive Newton's laws of motions from Lagrangian formulation? Incidentally, there is an even general principal in Physics called \"Principal of least action\". Although, I won't go into the physics of it here, the underlying math is a beautiful technique called Calculus of variations. Variational Calculus is concerned with finding an extremum of an integral. There can be multiple paths between the end points of the integral. This technique allows us to find the path where this integral is at its extremum, or put it differently, where small variation in the path, does not change the value of the integral. Source: https://www.storyofmathematics.com/18th.html Problem Statement \u00b6 We are given a functional J J (a fancy word for a mapping that maps a function to a scalar). So J J is a scalar value given by the integration of f(x, \\dot{x}, t) f(x, \\dot{x}, t) where t t is an independent variable. For now assume that x x is 1-dimensional. J = \\int_{a}^{b} f(x, \\dot{x}, t) dt J = \\int_{a}^{b} f(x, \\dot{x}, t) dt We are given f f is given to you, which exists 1 across the paths between a a and b b . So J J is a functional that maps x(t) x(t) to a scalar number (say, in \\mathbb{R} \\mathbb{R} ). We wish to find x(t) x(t) which finds the stational point of J J . So really, the equation should be rewritten as J(x(t)) = \\int_{a}^{b} f(x(t), \\dot{x}(t), t) dt J(x(t)) = \\int_{a}^{b} f(x(t), \\dot{x}(t), t) dt Where \\dot{x} = \\dot{x}(t) \\dot{x} = \\dot{x}(t) is a short hand for \\frac{dx(t)}{dt} \\frac{dx(t)}{dt} . Solution \u00b6 So how do we find the stational point of J J . In ordinal calculus, if we are to find a stationary point, say a minima, of a function f(x) f(x) , we can do that by equating f'(x) = 0 f'(x) = 0 . This is because, at a stationary point is one where a miniscule variation along x x does not change f(x) f(x) But the problem here is different. J(x(t)) J(x(t)) is not a function of independent variable t t but function of function x(t) x(t) and we don't know what x(t) x(t) . Different x(t) should result in different paths between t=a t=a and t=b t=b . And we don't know how to differentiate wrt a function now, do we? The trick is to change dependence of J J to a variable \\alpha \\alpha such that varying alpha in turn means plugging in different functions x(t) x(t) . To this end, lets write write an auxilliary function for x(t) x(t) g(t, \\alpha) = x(t) + \\alpha \\eta(t) g(t, \\alpha) = x(t) + \\alpha \\eta(t) At \\alpha = 0 \\alpha = 0 , x(t, \\alpha) x(t, \\alpha) reduces to x(t) x(t) . \\eta(t) \\eta(t) can be any arbitrary function of t t such that \\eta(a)=\\eta(b)=0 \\eta(a)=\\eta(b)=0 . Notice for a given $x(t), by varying \\alpha \\alpha and \\eta(t) \\eta(t) , one can obtain any path g(t, \\alpha) g(t, \\alpha) . So without loss of generality, we can write J(g(t, \\alpha)) = \\int_{a}^{b} f(g(t, \\alpha), \\dot{g}(t, \\alpha), t) dt J(g(t, \\alpha)) = \\int_{a}^{b} f(g(t, \\alpha), \\dot{g}(t, \\alpha), t) dt Now, the stationary point should exist where variation in the path should result in no change in J J . Assume, that x(t) x(t) . Then variation in x(t) x(t) is precisely given by varying alpha in g(t, \\alpha) g(t, \\alpha) . So we have managed to quantify this variation to a variable wrt which we know how to differentiate. Thus, to find the stationary point we should have \\frac{\\partial{J}}{\\partial{\\alpha}}) = 0 \\frac{\\partial{J}}{\\partial{\\alpha}}) = 0 \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f(g,\\dot{g},t)}}{\\partial{\\alpha}}dt \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f(g,\\dot{g},t)}}{\\partial{\\alpha}}dt We could move the derivative inside the integral since, the integral is wrt t t while the derivative is wrt \\alpha \\alpha . Expanding further, and applying chain rule, we have \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}}dt + \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}}dt + \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt We are going to need more mathematical juggling to simplify above. To this end, lets consider each of the addands inside the integral on the right hand side. The first term is easy, since \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) . \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}} dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\eta(t) dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}} dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\eta(t) dt As for second term, recall that \\dot{g} = \\frac{dg}{dt} \\dot{g} = \\frac{dg}{dt} , which yields \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial^2 {g}}{\\partial{t}\\partial{\\alpha}}dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial^2 {g}}{\\partial{t}\\partial{\\alpha}}dt Writing it this way, makes it look like we could use integration by part to further simplify this. \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\frac{\\partial{g}}{\\partial{\\alpha}} \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\frac{\\partial{g}}{\\partial{\\alpha}}dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\frac{\\partial{g}}{\\partial{\\alpha}} \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\frac{\\partial{g}}{\\partial{\\alpha}}dt Again, by substituing \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) , we get \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\eta(t) \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\eta(t) \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt Now, since we are only concerned with paths that start from a a and end in b b , we chose \\eta(t) \\eta(t) that vanishes on both of these point. Consequently, we can get rid of the first term \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt Substituting this result into our original equation, we get \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{g}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} ) \\eta(t)dt \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{g}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} ) \\eta(t)dt Again recall, that at \\alpha = 0 \\alpha = 0 , we the value of g(t, \\alpha = 0) = x(t) g(t, \\alpha = 0) = x(t) where x(t) x(t) is the path of stationary point for the functional J J . Therefore, we have \\frac{\\partial{J}}{\\partial{\\alpha}} \\bigg\\rvert_{\\alpha=0} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} ) \\eta(t)dt = 0 \\frac{\\partial{J}}{\\partial{\\alpha}} \\bigg\\rvert_{\\alpha=0} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} ) \\eta(t)dt = 0 Now we have situation, where for any arbitrary \\eta(t) \\eta(t) , the above integral is zero. This must 2 mean that \\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} = 0 \\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} = 0 which is the famous Euler-Lagrange Equation. exists means that f f is continuous and differentiable. Also, f f could have been dependent of higher order differentials if we choose, but I am restricting this discussion to a special case of Euler-Langrange equation only. \u21a9 The fundamental lemma of caculus of variation states that if \\int M(x)N(x) = 0 \\int M(x)N(x) = 0 for arbitrary N(x) N(x) , the M(x) M(x) must be zero. Intuitively, since integration is like summation, essentially, what one is saying is that is the production of two variables a a and b b is such that ab = 0 ab = 0 for arbitrary choice of b b , then a a must be zero. \u21a9","title":"Variational Calculus"},{"location":"Maths/variational_calculus/#calculus-of-variations-euler-lagrange-equation","text":"Did you know that you could derive Newton's laws of motions from Lagrangian formulation? Incidentally, there is an even general principal in Physics called \"Principal of least action\". Although, I won't go into the physics of it here, the underlying math is a beautiful technique called Calculus of variations. Variational Calculus is concerned with finding an extremum of an integral. There can be multiple paths between the end points of the integral. This technique allows us to find the path where this integral is at its extremum, or put it differently, where small variation in the path, does not change the value of the integral. Source: https://www.storyofmathematics.com/18th.html","title":"Calculus of variations: Euler-Lagrange Equation"},{"location":"Maths/variational_calculus/#problem-statement","text":"We are given a functional J J (a fancy word for a mapping that maps a function to a scalar). So J J is a scalar value given by the integration of f(x, \\dot{x}, t) f(x, \\dot{x}, t) where t t is an independent variable. For now assume that x x is 1-dimensional. J = \\int_{a}^{b} f(x, \\dot{x}, t) dt J = \\int_{a}^{b} f(x, \\dot{x}, t) dt We are given f f is given to you, which exists 1 across the paths between a a and b b . So J J is a functional that maps x(t) x(t) to a scalar number (say, in \\mathbb{R} \\mathbb{R} ). We wish to find x(t) x(t) which finds the stational point of J J . So really, the equation should be rewritten as J(x(t)) = \\int_{a}^{b} f(x(t), \\dot{x}(t), t) dt J(x(t)) = \\int_{a}^{b} f(x(t), \\dot{x}(t), t) dt Where \\dot{x} = \\dot{x}(t) \\dot{x} = \\dot{x}(t) is a short hand for \\frac{dx(t)}{dt} \\frac{dx(t)}{dt} .","title":"Problem Statement"},{"location":"Maths/variational_calculus/#solution","text":"So how do we find the stational point of J J . In ordinal calculus, if we are to find a stationary point, say a minima, of a function f(x) f(x) , we can do that by equating f'(x) = 0 f'(x) = 0 . This is because, at a stationary point is one where a miniscule variation along x x does not change f(x) f(x) But the problem here is different. J(x(t)) J(x(t)) is not a function of independent variable t t but function of function x(t) x(t) and we don't know what x(t) x(t) . Different x(t) should result in different paths between t=a t=a and t=b t=b . And we don't know how to differentiate wrt a function now, do we? The trick is to change dependence of J J to a variable \\alpha \\alpha such that varying alpha in turn means plugging in different functions x(t) x(t) . To this end, lets write write an auxilliary function for x(t) x(t) g(t, \\alpha) = x(t) + \\alpha \\eta(t) g(t, \\alpha) = x(t) + \\alpha \\eta(t) At \\alpha = 0 \\alpha = 0 , x(t, \\alpha) x(t, \\alpha) reduces to x(t) x(t) . \\eta(t) \\eta(t) can be any arbitrary function of t t such that \\eta(a)=\\eta(b)=0 \\eta(a)=\\eta(b)=0 . Notice for a given $x(t), by varying \\alpha \\alpha and \\eta(t) \\eta(t) , one can obtain any path g(t, \\alpha) g(t, \\alpha) . So without loss of generality, we can write J(g(t, \\alpha)) = \\int_{a}^{b} f(g(t, \\alpha), \\dot{g}(t, \\alpha), t) dt J(g(t, \\alpha)) = \\int_{a}^{b} f(g(t, \\alpha), \\dot{g}(t, \\alpha), t) dt Now, the stationary point should exist where variation in the path should result in no change in J J . Assume, that x(t) x(t) . Then variation in x(t) x(t) is precisely given by varying alpha in g(t, \\alpha) g(t, \\alpha) . So we have managed to quantify this variation to a variable wrt which we know how to differentiate. Thus, to find the stationary point we should have \\frac{\\partial{J}}{\\partial{\\alpha}}) = 0 \\frac{\\partial{J}}{\\partial{\\alpha}}) = 0 \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f(g,\\dot{g},t)}}{\\partial{\\alpha}}dt \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f(g,\\dot{g},t)}}{\\partial{\\alpha}}dt We could move the derivative inside the integral since, the integral is wrt t t while the derivative is wrt \\alpha \\alpha . Expanding further, and applying chain rule, we have \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}}dt + \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}}dt + \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt We are going to need more mathematical juggling to simplify above. To this end, lets consider each of the addands inside the integral on the right hand side. The first term is easy, since \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) . \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}} dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\eta(t) dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\frac{\\partial{g}}{\\partial{\\alpha}} dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{g}}\\eta(t) dt As for second term, recall that \\dot{g} = \\frac{dg}{dt} \\dot{g} = \\frac{dg}{dt} , which yields \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial^2 {g}}{\\partial{t}\\partial{\\alpha}}dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial^2 {g}}{\\partial{t}\\partial{\\alpha}}dt Writing it this way, makes it look like we could use integration by part to further simplify this. \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\frac{\\partial{g}}{\\partial{\\alpha}} \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\frac{\\partial{g}}{\\partial{\\alpha}}dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\frac{\\partial{g}}{\\partial{\\alpha}} \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\frac{\\partial{g}}{\\partial{\\alpha}}dt Again, by substituing \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) \\frac{\\partial{g}}{\\partial{\\alpha}} = \\eta(t) , we get \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\eta(t) \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = \\frac{\\partial{f}}{\\partial{\\dot{g}}} \\eta(t) \\bigg\\rvert_{a}^{b} - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt Now, since we are only concerned with paths that start from a a and end in b b , we chose \\eta(t) \\eta(t) that vanishes on both of these point. Consequently, we can get rid of the first term \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt \\int_{a}^{b} \\frac{\\partial{f}}{\\partial{\\dot{g}}}\\frac{\\partial{\\dot{g}}}{\\partial{\\alpha}}dt = - \\int_{a}^{b} \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} \\eta(t)dt Substituting this result into our original equation, we get \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{g}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} ) \\eta(t)dt \\frac{\\partial{J}}{\\partial{\\alpha}} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{g}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{g}}})}{dt} ) \\eta(t)dt Again recall, that at \\alpha = 0 \\alpha = 0 , we the value of g(t, \\alpha = 0) = x(t) g(t, \\alpha = 0) = x(t) where x(t) x(t) is the path of stationary point for the functional J J . Therefore, we have \\frac{\\partial{J}}{\\partial{\\alpha}} \\bigg\\rvert_{\\alpha=0} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} ) \\eta(t)dt = 0 \\frac{\\partial{J}}{\\partial{\\alpha}} \\bigg\\rvert_{\\alpha=0} = \\int_{a}^{b} (\\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} ) \\eta(t)dt = 0 Now we have situation, where for any arbitrary \\eta(t) \\eta(t) , the above integral is zero. This must 2 mean that \\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} = 0 \\frac{\\partial{f}}{\\partial{x}} - \\frac{d(\\frac{\\partial{f}}{\\partial{\\dot{x}}})}{dt} = 0 which is the famous Euler-Lagrange Equation. exists means that f f is continuous and differentiable. Also, f f could have been dependent of higher order differentials if we choose, but I am restricting this discussion to a special case of Euler-Langrange equation only. \u21a9 The fundamental lemma of caculus of variation states that if \\int M(x)N(x) = 0 \\int M(x)N(x) = 0 for arbitrary N(x) N(x) , the M(x) M(x) must be zero. Intuitively, since integration is like summation, essentially, what one is saying is that is the production of two variables a a and b b is such that ab = 0 ab = 0 for arbitrary choice of b b , then a a must be zero. \u21a9","title":"Solution"},{"location":"TensorAlgebra/Chap1/","text":"Chapter 1 \u00b6 Welcome to this blog series on Tensor Algebra. This blog series is my attempt to explain Tensor Algebra as I learnt from various sources over the Internet. This chapter is mostly to motivate why you might want to study Tensor Algebra. Pre-requisite \u00b6 This blog series assumes a familiarity with Linear Algebra (concepts like matrix-multiplication, dot products, linear combination, etc.) This blog should be particularly helpful for those who never had formal college degree in Physics and as such find it hard to delve into certain subjects where Tensors are pre-requisite. No knowledge of calculus is needed to study Tensor Algebra. I may do a follow up series on Tensor Calculus though. Why would you want to study Tensor Algebra? \u00b6 Most people first hear about Tensor Algebra when they start out their journey to delve deeper into the General Theory of Relatively (GTR) . In GTR, Eistein shows how space-time forms a single object which is curved by massive objects ( and other crazy ideas like blackholes and gravitational waves). Space time curves near massive objects as predicted by General Theory of Relativity, but what does that even mean? You might have also heard of big bang and how universe is expanding since. Tensors are an indispensable tool to understand what these mean mathematically. What does it really mean mathematically when people say Universe is expanding? Another popular example where tensors come up often is Quantum Mechanics(QM) and Quantum Computing(QC) , In QM, the mystical concept of Quantum Superposition where in a particles can remain in multiple states until it is observed is essential linear combination of tensors which we will be cover later. Quantum Superposition: Is the cat dead or alive? You may have heard that nothing travels faster than speed of light, yet if you separate two particles whose states are entangled, simple act of observing one instantaneously affects the other. Again, this is mathematically represented by tensor product which we will cover as well later in this series. \"Spooky action at a distance\"- Einstein. Huh? There is probably nothing more intriguing and fascinating than the unintuitive world of small scale (QM) as well as mind warping idea that time isn't same for everyone or that our universe is expanding. Tensors is a beautiful theory that allows one to study the underlying (complicated) Geometry of Space-time, something that one cannot get just by watching popular science articles or documentaries on these topics. Alfred North Whitehead said The idea that physicists would in future have to study the theory of tensors created real panic amongst them following the first announcement that Eistein's predictions had been verified. However, GTR and QM are not the only subjects tensors are used in. Tensors are prevalent in many domains of engineering and science, and good understanding of tensors would build a solid foundation for learning numerous other STEM subjects. What are Tensors? \u00b6 It is hard to explain what Tensor is. Different people tend to give different definition of tensor. And whats worse is that most of them are partially correct, but none completely. There are generally three ways to think about Tensor. Tensors are multi-dimensional arrays. An array is a list of numbers. If you replicate that list multiple times, like a table or MS excel, than it is called a 2-D array and so on and so forth. For example, you may have heard of popular scientific computation library called TensorFlow , which is all about manipulation of multi-dimensional arrays. Tensors are characterised by rank, as follows: Scalar (rank 0 tensor): [5], [1], [2.5], [\\pi] [5], [1], [2.5], [\\pi] Vector (rank 1 tensor): [1, 2, 3] [1, 2, 3] , [0.1, 3, \\sqrt{2}] [0.1, 3, \\sqrt{2}] Matrix (rank 2 tensor): \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} , \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} So on for rank 3, 4, ... This definition is incorrect Because while, Tensors can be represented as multi-dimensional arrays , tensors as concept are much more than just bunch of numbers. While calling multidimensional arrays tensor is generally accepted in Machine Learning community, it is not strictly correct. Tensors have geometrical meaning which is not apparent in this definition. Tensor is an object that is invariant under a change of coordinates . Under this definition, Tensors as object do not change when viewed from different coordinate system. As the choice of coordinate system changes (say Castersian to Polar), the representation of the same tensor in terms of the coordinate does change, however. The change is predictible allowing us to go back and forth from one coordinate system to another using known transformation rules.","title":"Chapter 1"},{"location":"TensorAlgebra/Chap1/#chapter-1","text":"Welcome to this blog series on Tensor Algebra. This blog series is my attempt to explain Tensor Algebra as I learnt from various sources over the Internet. This chapter is mostly to motivate why you might want to study Tensor Algebra.","title":"Chapter 1"},{"location":"TensorAlgebra/Chap1/#pre-requisite","text":"This blog series assumes a familiarity with Linear Algebra (concepts like matrix-multiplication, dot products, linear combination, etc.) This blog should be particularly helpful for those who never had formal college degree in Physics and as such find it hard to delve into certain subjects where Tensors are pre-requisite. No knowledge of calculus is needed to study Tensor Algebra. I may do a follow up series on Tensor Calculus though.","title":"Pre-requisite"},{"location":"TensorAlgebra/Chap1/#why-would-you-want-to-study-tensor-algebra","text":"Most people first hear about Tensor Algebra when they start out their journey to delve deeper into the General Theory of Relatively (GTR) . In GTR, Eistein shows how space-time forms a single object which is curved by massive objects ( and other crazy ideas like blackholes and gravitational waves). Space time curves near massive objects as predicted by General Theory of Relativity, but what does that even mean? You might have also heard of big bang and how universe is expanding since. Tensors are an indispensable tool to understand what these mean mathematically. What does it really mean mathematically when people say Universe is expanding? Another popular example where tensors come up often is Quantum Mechanics(QM) and Quantum Computing(QC) , In QM, the mystical concept of Quantum Superposition where in a particles can remain in multiple states until it is observed is essential linear combination of tensors which we will be cover later. Quantum Superposition: Is the cat dead or alive? You may have heard that nothing travels faster than speed of light, yet if you separate two particles whose states are entangled, simple act of observing one instantaneously affects the other. Again, this is mathematically represented by tensor product which we will cover as well later in this series. \"Spooky action at a distance\"- Einstein. Huh? There is probably nothing more intriguing and fascinating than the unintuitive world of small scale (QM) as well as mind warping idea that time isn't same for everyone or that our universe is expanding. Tensors is a beautiful theory that allows one to study the underlying (complicated) Geometry of Space-time, something that one cannot get just by watching popular science articles or documentaries on these topics. Alfred North Whitehead said The idea that physicists would in future have to study the theory of tensors created real panic amongst them following the first announcement that Eistein's predictions had been verified. However, GTR and QM are not the only subjects tensors are used in. Tensors are prevalent in many domains of engineering and science, and good understanding of tensors would build a solid foundation for learning numerous other STEM subjects.","title":"Why would you want to study Tensor Algebra?"},{"location":"TensorAlgebra/Chap1/#what-are-tensors","text":"It is hard to explain what Tensor is. Different people tend to give different definition of tensor. And whats worse is that most of them are partially correct, but none completely. There are generally three ways to think about Tensor. Tensors are multi-dimensional arrays. An array is a list of numbers. If you replicate that list multiple times, like a table or MS excel, than it is called a 2-D array and so on and so forth. For example, you may have heard of popular scientific computation library called TensorFlow , which is all about manipulation of multi-dimensional arrays. Tensors are characterised by rank, as follows: Scalar (rank 0 tensor): [5], [1], [2.5], [\\pi] [5], [1], [2.5], [\\pi] Vector (rank 1 tensor): [1, 2, 3] [1, 2, 3] , [0.1, 3, \\sqrt{2}] [0.1, 3, \\sqrt{2}] Matrix (rank 2 tensor): \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} , \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} So on for rank 3, 4, ... This definition is incorrect Because while, Tensors can be represented as multi-dimensional arrays , tensors as concept are much more than just bunch of numbers. While calling multidimensional arrays tensor is generally accepted in Machine Learning community, it is not strictly correct. Tensors have geometrical meaning which is not apparent in this definition. Tensor is an object that is invariant under a change of coordinates . Under this definition, Tensors as object do not change when viewed from different coordinate system. As the choice of coordinate system changes (say Castersian to Polar), the representation of the same tensor in terms of the coordinate does change, however. The change is predictible allowing us to go back and forth from one coordinate system to another using known transformation rules.","title":"What are Tensors?"},{"location":"Universe/computation_speculation/","text":"Speculation: Computation, Information, Conciousness and Theory of Everything in full circle \u00b6 In my quest to improve AI, I have been very interested in understanding the fundamental nature of computation. Neural networks as we know are black boxes and while we know that they \u201ccompute\u201d, what really goes inside a network remains a mystery. In theory of computation, we define specific type of computation by a language which is a set of string from built from some alphabet. All of primitive/analytical models of computing (automata, pushdown automata, and Turing machines ) that accept the language, can essentially tell apart (by accepting or rejecting) strings that belong to the language from those that don\u2019t. In essence, these models have reduced all types of computations to answering \u201cyes\u201d or \u201cno\u201d / \u201caccept\u201d or \u201creject\u201d / \u201con\u201d or \u201coff\u201d / \u201c1\u201d or \u201c0\u201d. Let me explain how. Here is the clever insight. This is very analogous to mathematical definition of a function, where function is a relation ( a predicate with two variables). It is a cross product of a set of elements in domain and range, assigning truth value to each tuple. Now the colloquial way to think of computation is usually a black box that takes in an element from domain and spits out an element from range. Clearly a function \u201ccomputes\u201d. So the mathematical definition of function as a relation, is an alternate model / form of computation. In other words they are equivalent. This also relates computation with information in terms of bits (truth or false). In theory of computation, we are not concerned about the efficiency of the computation. That is, it doesn\u2019t matter if the computation takes infinite time, as long as it terminates at some point. This is apparent in the \u201crelation\u201d definition of function, where one can assume given such a relation, one can iterate over all (albeit infinite sometimes) pairs of tuples (from domain, and range) and be able to perform any computation from this function. In this view, the tools of mathematics such analytical form of function (like y = log(x)) is just for efficiency purposes, but does not change the nature of computation. Given that in order to compute, one can iterate over infinite tuples, it seems plausible (perhaps for likes of Einstein) to come up with a thought experiment involving elementary particles to define equivalence between work and computation. To best of my knowledge, I didn\u2019t know of any such equivalence. So I googled! It seems that there is a 2010 paper which defines equivalence between information and energy. This is encouraging even though \u201cinformation\" is not quite the same as \u201ccomputation\u201d. There is increased hype around \u201cinformation\" being the fundamental element in the universe. One of the arguments about the equivalence of information to energy is described by Maxwell\u2019s demon. In this experiment, the demon is able to open and close slits between two gas chambers. The demon (because she is a demon) is assumed to know the state (velocity, position, etc) of all particles in the gas on both sides of the chamber. Now in principle, the demon can use this information to separate out all particles of high energy to one side and low energy on the other, creating a temperature gradient. And we already know that such a system has energy (think steam engines). Did the demon create energy out of nothing? The entropy must increase, and we can attribute this to the computation that goes inside demon\u2019s head. Some proponents of this idea (information being the primitive entity) cite that the maximum amount of information that a blackhole can have is bounded. I don\u2019t understand the theory behind this, but they are able to connect entropy of a black hole to the area of its event horizon and this limiting the amount of information that can be encapsulated within it, or something to that effect. All elementary particles seem to have two different values of spin (call it truth values or bits). And when they interact, there is a predefined manner in which the spin of the resulting system turns out. So in some sense, these elementary particles are doing logical operation at microscopic scale. No one has actually seen fundamental particles, and they are really just abstractions that help to visualise ultimately what computations take place by the laws of the universe. Therefore, my view is that all the theories that we have are different perspective/abstractions to conceptualise the underlying computation within the universe. Classical Mechanics did it, and then GR, then Quantum Mechanics and QFT. I wouldn\u2019t be surprised if we get another theory, perhaps better theory which encapsulates computation that is captured by all previous theory and more (theory of everything). And computation is nothing but manipulation of information. Now time for my wild speculation. According to our best quantum theory, the effect of \u201cobservation\u201d by a conscious being seems to affect how the universe renders itself. Could we think of consciousness in terms of information and find a connection between the two? Let\u2019s try. For double split experiment, when no observation is made, the information (as defined by Shannon) is high due to uncertainty, there is equal probability of electron passing through any of the slits. However, the act of observation immediately drops the information content to zero. After observation, there is no uncertainty about the location of the electron. It is as if the information transferred from the system to the mind of the conscious observer. So could we come up with a theory where we argue that information in the universe remains conserved, and it only gets transferred. Continuing further with this idea, what really is observation? No matter how the conscious being observed (deterministic or probabilistically), the observer must do \u201ccomputation\u201d to understand what really just happened. Thus, observation is nothing but computation? If this were true, then we can think of computation as a medium of information transfer within the universe. Since the observer is really part of the universe, there is nothing special about being conscious. I say there is nothing special about being consciousness because in above argument, the special status of being conscious in quantum mechanics is due to the ability of the conscious being too \u201ccompute\u201d. The universe would behave in the same way (collapse wave function, etc) if there was a modern computer which could observe the experiment and then display \u201cLeft\u201d or \u201cRight\u201d. I think this is important point to dwell on for a bit. A modern computer would suffice as an observer in a double split experiment. I think I am more convinced about this last part because inarguably we believe humans are conscious, and we know for a fact that humans evolved over thousands of years from even simpler beings to basic amino acids which were clearly not \u201cconscious\u201d in the mystical sense associated with it. I think it is worth exploring the connection between computation as a means to transfer of information between connected systems of the universe. Entropy on the other hand is the side effect of any act of computation (think again of Maxwell's demon). We know that the entropy of the universe increases, but above hypothesis possibly lets us understand that it is the underlying \u201ccomputation\u201d that is necessary ingredient. We typically think of entropy in terms of statistical likelihoods. An ordered state is exponentially less likely than any other random unordered state. Even in this view, for the system to get from one state to another, some form of computation must take place. I know there is one glaring loophole in this idea of computation as medium of information transfer. In we agree with Shannon\u2019s definition of information as a measurement of uncertainty, then it is not clear what information transfer really means mathematically. In double split experiment, I argued that the information content dropped down to zero by act of observation (aka computation), but how do we quantify that this information is transferred to the observer? The observer was uncertain about the location of electron until observation. So it seems that the drop in information content (in Shannon\u2019s sense) of the observed system is equal to the information gained (in colloquial english sense) by the observer. May be that is what defines a conscious being after all. When two conscious beings interact, they exchange information. A teacher conveys information to a student which uses neural computation to decrease uncertainty about the concept learnt. Going full circle then, we can define computation as a process of reducing uncertainty of the system. This fits well with the mathematical definition of the function as well. Before computation, we knew about what elements there are in domain and range sets (which can hypothetically include everything) and there were combinatorially many possibilities (each element is set of domain could map to any element of range). The act of computation helped reduce uncertainty. For example, for a parabola (u=4x^2) on a real plane, before computation, x=1 could be associated with any of infinite values of y. However, computation deterministically reduced the uncertainty to zero. My knowledge of physics, world or anything else for that matter, is very limited compared to great thinkers of the past and present and I am also aware that my speculation is more likely to be wrong than it is right. Let me know what you think, I would love to be shown flaws in my thinking or my outright leaps of faith.","title":"Computation, Information, Conciousness and Theory of Everything in full circle"},{"location":"Universe/computation_speculation/#speculation-computation-information-conciousness-and-theory-of-everything-in-full-circle","text":"In my quest to improve AI, I have been very interested in understanding the fundamental nature of computation. Neural networks as we know are black boxes and while we know that they \u201ccompute\u201d, what really goes inside a network remains a mystery. In theory of computation, we define specific type of computation by a language which is a set of string from built from some alphabet. All of primitive/analytical models of computing (automata, pushdown automata, and Turing machines ) that accept the language, can essentially tell apart (by accepting or rejecting) strings that belong to the language from those that don\u2019t. In essence, these models have reduced all types of computations to answering \u201cyes\u201d or \u201cno\u201d / \u201caccept\u201d or \u201creject\u201d / \u201con\u201d or \u201coff\u201d / \u201c1\u201d or \u201c0\u201d. Let me explain how. Here is the clever insight. This is very analogous to mathematical definition of a function, where function is a relation ( a predicate with two variables). It is a cross product of a set of elements in domain and range, assigning truth value to each tuple. Now the colloquial way to think of computation is usually a black box that takes in an element from domain and spits out an element from range. Clearly a function \u201ccomputes\u201d. So the mathematical definition of function as a relation, is an alternate model / form of computation. In other words they are equivalent. This also relates computation with information in terms of bits (truth or false). In theory of computation, we are not concerned about the efficiency of the computation. That is, it doesn\u2019t matter if the computation takes infinite time, as long as it terminates at some point. This is apparent in the \u201crelation\u201d definition of function, where one can assume given such a relation, one can iterate over all (albeit infinite sometimes) pairs of tuples (from domain, and range) and be able to perform any computation from this function. In this view, the tools of mathematics such analytical form of function (like y = log(x)) is just for efficiency purposes, but does not change the nature of computation. Given that in order to compute, one can iterate over infinite tuples, it seems plausible (perhaps for likes of Einstein) to come up with a thought experiment involving elementary particles to define equivalence between work and computation. To best of my knowledge, I didn\u2019t know of any such equivalence. So I googled! It seems that there is a 2010 paper which defines equivalence between information and energy. This is encouraging even though \u201cinformation\" is not quite the same as \u201ccomputation\u201d. There is increased hype around \u201cinformation\" being the fundamental element in the universe. One of the arguments about the equivalence of information to energy is described by Maxwell\u2019s demon. In this experiment, the demon is able to open and close slits between two gas chambers. The demon (because she is a demon) is assumed to know the state (velocity, position, etc) of all particles in the gas on both sides of the chamber. Now in principle, the demon can use this information to separate out all particles of high energy to one side and low energy on the other, creating a temperature gradient. And we already know that such a system has energy (think steam engines). Did the demon create energy out of nothing? The entropy must increase, and we can attribute this to the computation that goes inside demon\u2019s head. Some proponents of this idea (information being the primitive entity) cite that the maximum amount of information that a blackhole can have is bounded. I don\u2019t understand the theory behind this, but they are able to connect entropy of a black hole to the area of its event horizon and this limiting the amount of information that can be encapsulated within it, or something to that effect. All elementary particles seem to have two different values of spin (call it truth values or bits). And when they interact, there is a predefined manner in which the spin of the resulting system turns out. So in some sense, these elementary particles are doing logical operation at microscopic scale. No one has actually seen fundamental particles, and they are really just abstractions that help to visualise ultimately what computations take place by the laws of the universe. Therefore, my view is that all the theories that we have are different perspective/abstractions to conceptualise the underlying computation within the universe. Classical Mechanics did it, and then GR, then Quantum Mechanics and QFT. I wouldn\u2019t be surprised if we get another theory, perhaps better theory which encapsulates computation that is captured by all previous theory and more (theory of everything). And computation is nothing but manipulation of information. Now time for my wild speculation. According to our best quantum theory, the effect of \u201cobservation\u201d by a conscious being seems to affect how the universe renders itself. Could we think of consciousness in terms of information and find a connection between the two? Let\u2019s try. For double split experiment, when no observation is made, the information (as defined by Shannon) is high due to uncertainty, there is equal probability of electron passing through any of the slits. However, the act of observation immediately drops the information content to zero. After observation, there is no uncertainty about the location of the electron. It is as if the information transferred from the system to the mind of the conscious observer. So could we come up with a theory where we argue that information in the universe remains conserved, and it only gets transferred. Continuing further with this idea, what really is observation? No matter how the conscious being observed (deterministic or probabilistically), the observer must do \u201ccomputation\u201d to understand what really just happened. Thus, observation is nothing but computation? If this were true, then we can think of computation as a medium of information transfer within the universe. Since the observer is really part of the universe, there is nothing special about being conscious. I say there is nothing special about being consciousness because in above argument, the special status of being conscious in quantum mechanics is due to the ability of the conscious being too \u201ccompute\u201d. The universe would behave in the same way (collapse wave function, etc) if there was a modern computer which could observe the experiment and then display \u201cLeft\u201d or \u201cRight\u201d. I think this is important point to dwell on for a bit. A modern computer would suffice as an observer in a double split experiment. I think I am more convinced about this last part because inarguably we believe humans are conscious, and we know for a fact that humans evolved over thousands of years from even simpler beings to basic amino acids which were clearly not \u201cconscious\u201d in the mystical sense associated with it. I think it is worth exploring the connection between computation as a means to transfer of information between connected systems of the universe. Entropy on the other hand is the side effect of any act of computation (think again of Maxwell's demon). We know that the entropy of the universe increases, but above hypothesis possibly lets us understand that it is the underlying \u201ccomputation\u201d that is necessary ingredient. We typically think of entropy in terms of statistical likelihoods. An ordered state is exponentially less likely than any other random unordered state. Even in this view, for the system to get from one state to another, some form of computation must take place. I know there is one glaring loophole in this idea of computation as medium of information transfer. In we agree with Shannon\u2019s definition of information as a measurement of uncertainty, then it is not clear what information transfer really means mathematically. In double split experiment, I argued that the information content dropped down to zero by act of observation (aka computation), but how do we quantify that this information is transferred to the observer? The observer was uncertain about the location of electron until observation. So it seems that the drop in information content (in Shannon\u2019s sense) of the observed system is equal to the information gained (in colloquial english sense) by the observer. May be that is what defines a conscious being after all. When two conscious beings interact, they exchange information. A teacher conveys information to a student which uses neural computation to decrease uncertainty about the concept learnt. Going full circle then, we can define computation as a process of reducing uncertainty of the system. This fits well with the mathematical definition of the function as well. Before computation, we knew about what elements there are in domain and range sets (which can hypothetically include everything) and there were combinatorially many possibilities (each element is set of domain could map to any element of range). The act of computation helped reduce uncertainty. For example, for a parabola (u=4x^2) on a real plane, before computation, x=1 could be associated with any of infinite values of y. However, computation deterministically reduced the uncertainty to zero. My knowledge of physics, world or anything else for that matter, is very limited compared to great thinkers of the past and present and I am also aware that my speculation is more likely to be wrong than it is right. Let me know what you think, I would love to be shown flaws in my thinking or my outright leaps of faith.","title":"Speculation: Computation, Information, Conciousness and Theory of Everything in full circle"},{"location":"notes/notes/","text":"","title":"Notes"}]}