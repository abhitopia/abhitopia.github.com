{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"If you can\u2019t explain it simply, you don\u2019t understand it well enough. - Albert Einstein The purpose of this blog is two fold Learn better by following Feynman Technique Organize plethora of information I deem worth preserving for purpose of knowledge sharing and recall. The blog is broadly organized into following cateories: Tensor Algebra \u00b6 A short course (WIP) to build solid foundation in Tensor Algebra, which is a prerequisite to Tensor Calculus, which itself is a prerequisite to Differential Geometry, which in turn forms the mathematical language for Einstein's General Theory of Relativity. Machine Learning \u00b6 Different Machine learning topics, including an archive of interesting papers . Contact \u00b6 I am always looking for like-minded people and chances are, if you are reading this blog, you are one of them. If you are interested in giving feedback, provide new or discuss existing content, or just want to say hi, you can reach out to me at Email: abhi at abhitopia . com Twitter: abhitopia","title":""},{"location":"#tensor-algebra","text":"A short course (WIP) to build solid foundation in Tensor Algebra, which is a prerequisite to Tensor Calculus, which itself is a prerequisite to Differential Geometry, which in turn forms the mathematical language for Einstein's General Theory of Relativity.","title":"Tensor Algebra"},{"location":"#machine-learning","text":"Different Machine learning topics, including an archive of interesting papers .","title":"Machine Learning"},{"location":"#contact","text":"I am always looking for like-minded people and chances are, if you are reading this blog, you are one of them. If you are interested in giving feedback, provide new or discuss existing content, or just want to say hi, you can reach out to me at Email: abhi at abhitopia . com Twitter: abhitopia","title":"Contact"},{"location":"ML/Concepts/fast_weights/","text":"I read a sunday classic References \u00b6 Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets.","title":"Fast-Weights in RNN, from first principles"},{"location":"ML/Concepts/fast_weights/#references","text":"Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets.","title":"References"},{"location":"ML/Concepts/misc/","text":"RNN training methods and corresponding computational complexity \u00b6 Paper Short term memory for RNNs \u00b6 This paper propose by Jimmy Ba and Hinton, proposes an old idea of using Hebbian like learning rule on 2 nd order TPR representations to augment traditional RNN to incoorporate temporary, short-term memory. Paper Fast weight memories Video by Jimmy Video by Hinton My two cents \u00b6 Can this be combined with Neural Ordinary Differential equation to determine automatically determine the length of inner loop? Can decay parameter be learning, than have constant exponentail rate decay? It will be interesting to see how large scape Ulmfit like language model trained using truncated BPTT will do. If it works, it should be faster (and real time) during inference compared to attentional variants including Transformer XL. Symbolic Neural Reasoning using TPR \u00b6 This follows on Paul Smolensky work on symbolic reasoning using TPR and gives an end-to-end learning framework using Fast-Weights update of third order TPRs. Very interesting paper, which scope to build upon further. Paper My two cents \u00b6 Authors haven't been able to integrate it to a full fledged LSTM (or unable to make it work). The idea is novel and needs further investigation. Graph Neural Networks \u00b6 Paper Paper Blog Neural and Symbolic Reasoning \u00b6 Workshop Course FOL Theorem Proving Dataset HolStep Dataset DeepMath - Deep Sequence Models for Premise Selection My two cents \u00b6 Theorem proving and theorem validation is epitomy of symbolic reasoning. It would be interesting to see how the Fast-weights and TPR variants above do on these tasks.","title":"Misc topics and papers"},{"location":"ML/Concepts/misc/#rnn-training-methods-and-corresponding-computational-complexity","text":"Paper","title":"RNN training methods and corresponding computational complexity"},{"location":"ML/Concepts/misc/#short-term-memory-for-rnns","text":"This paper propose by Jimmy Ba and Hinton, proposes an old idea of using Hebbian like learning rule on 2 nd order TPR representations to augment traditional RNN to incoorporate temporary, short-term memory. Paper Fast weight memories Video by Jimmy Video by Hinton","title":"Short term memory for RNNs"},{"location":"ML/Concepts/misc/#my-two-cents","text":"Can this be combined with Neural Ordinary Differential equation to determine automatically determine the length of inner loop? Can decay parameter be learning, than have constant exponentail rate decay? It will be interesting to see how large scape Ulmfit like language model trained using truncated BPTT will do. If it works, it should be faster (and real time) during inference compared to attentional variants including Transformer XL.","title":"My two cents"},{"location":"ML/Concepts/misc/#symbolic-neural-reasoning-using-tpr","text":"This follows on Paul Smolensky work on symbolic reasoning using TPR and gives an end-to-end learning framework using Fast-Weights update of third order TPRs. Very interesting paper, which scope to build upon further. Paper","title":"Symbolic Neural Reasoning using TPR"},{"location":"ML/Concepts/misc/#my-two-cents_1","text":"Authors haven't been able to integrate it to a full fledged LSTM (or unable to make it work). The idea is novel and needs further investigation.","title":"My two cents"},{"location":"ML/Concepts/misc/#graph-neural-networks","text":"Paper Paper Blog","title":"Graph Neural Networks"},{"location":"ML/Concepts/misc/#neural-and-symbolic-reasoning","text":"Workshop Course FOL Theorem Proving Dataset HolStep Dataset DeepMath - Deep Sequence Models for Premise Selection","title":"Neural and Symbolic Reasoning"},{"location":"ML/Concepts/misc/#my-two-cents_2","text":"Theorem proving and theorem validation is epitomy of symbolic reasoning. It would be interesting to see how the Fast-weights and TPR variants above do on these tasks.","title":"My two cents"},{"location":"TensorAlgebra/Chap1/","text":"Chapter 1 \u00b6 Welcome to this blog series on Tensor Algebra. This blog series is my attempt to explain Tensor Algebra as I learnt from various sources over the Internet. This chapter is mostly to motivate why you might want to study Tensor Algebra. Pre-requisite \u00b6 This blog series assumes a familiarity with Linear Algebra (concepts like matrix-multiplication, dot products, linear combination, etc.) This blog should be particularly helpful for those who never had formal college degree in Physics and as such find it hard to delve into certain subjects where Tensors are pre-requisite. No knowledge of calculus is needed to study Tensor Algebra. I may do a follow up series on Tensor Calculus though. Why would you want to study Tensor Algebra? \u00b6 Most people first hear about Tensor Algebra when they start out their journey to delve deeper into the General Theory of Relatively (GTR) . In GTR, Eistein shows how space-time forms a single object which is curved by massive objects ( and other crazy ideas like blackholes and gravitational waves). Space time curves near massive objects as predicted by General Theory of Relativity, but what does that even mean? You might have also heard of big bang and how universe is expanding since. Tensors are an indispensable tool to understand what these mean mathematically. What does it really mean mathematically when people say Universe is expanding? Another popular example where tensors come up often is Quantum Mechanics(QM) and Quantum Computing(QC) , In QM, the mystical concept of Quantum Superposition where in a particles can remain in multiple states until it is observed is essential linear combination of tensors which we will be cover later. Quantum Superposition: Is the cat dead or alive? You may have heard that nothing travels faster than speed of light, yet if you separate two particles whose states are entangled, simple act of observing one instantaneously affects the other. Again, this is mathematically represented by tensor product which we will cover as well later in this series. \"Spooky action at a distance\"- Einstein. Huh? There is probably nothing more intriguing and fascinating than the unintuitive world of small scale (QM) as well as mind warping idea that time isn't same for everyone or that our universe is expanding. Tensors is a beautiful theory that allows one to study the underlying (complicated) Geometry of Space-time, something that one cannot get just by watching popular science articles or documentaries on these topics. Alfred North Whitehead said The idea that physicists would in future have to study the theory of tensors created real panic amongst them following the first announcement that Eistein's predictions had been verified. However, GTR and QM are not the only subjects tensors are used in. Tensors are prevalent in many domains of engineering and science, and good understanding of tensors would build a solid foundation for learning numerous other STEM subjects. What are Tensors? \u00b6 It is hard to explain what Tensor is. Different people tend to give different definition of tensor. And whats worse is that most of them are partially correct, but none completely. There are generally three ways to think about Tensor. Tensors are multi-dimensional arrays. An array is a list of numbers. If you replicate that list multiple times, like a table or MS excel, than it is called a 2-D array and so on and so forth. For example, you may have heard of popular scientific computation library called TensorFlow , which is all about manipulation of multi-dimensional arrays. Tensors are characterised by rank, as follows: Scalar (rank 0 tensor): [5], [1], [2.5], [\\pi] [5], [1], [2.5], [\\pi] Vector (rank 1 tensor): [1, 2, 3] [1, 2, 3] , [0.1, 3, \\sqrt{2}] [0.1, 3, \\sqrt{2}] Matrix (rank 2 tensor): \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} , \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} So on for rank 3, 4, ... This definition is incorrect Because while, Tensors can be represented as multi-dimensional arrays , tensors as concept are much more than just bunch of numbers. While calling multidimensional arrays tensor is generally accepted in Machine Learning community, it is not strictly correct. Tensors have geometrical meaning which is not apparent in this definition. Tensor is an object that is invariant under a change of coordinates . Under this definition, Tensors as object do not change when viewed from different coordinate system. As the choice of coordinate system changes (say Castersian to Polar), the representation of the same tensor in terms of the coordinate does change, however. The change is predictible allowing us to go back and forth from one coordinate system to another using known transformation rules.","title":"Chapter 1"},{"location":"TensorAlgebra/Chap1/#chapter-1","text":"Welcome to this blog series on Tensor Algebra. This blog series is my attempt to explain Tensor Algebra as I learnt from various sources over the Internet. This chapter is mostly to motivate why you might want to study Tensor Algebra.","title":"Chapter 1"},{"location":"TensorAlgebra/Chap1/#pre-requisite","text":"This blog series assumes a familiarity with Linear Algebra (concepts like matrix-multiplication, dot products, linear combination, etc.) This blog should be particularly helpful for those who never had formal college degree in Physics and as such find it hard to delve into certain subjects where Tensors are pre-requisite. No knowledge of calculus is needed to study Tensor Algebra. I may do a follow up series on Tensor Calculus though.","title":"Pre-requisite"},{"location":"TensorAlgebra/Chap1/#why-would-you-want-to-study-tensor-algebra","text":"Most people first hear about Tensor Algebra when they start out their journey to delve deeper into the General Theory of Relatively (GTR) . In GTR, Eistein shows how space-time forms a single object which is curved by massive objects ( and other crazy ideas like blackholes and gravitational waves). Space time curves near massive objects as predicted by General Theory of Relativity, but what does that even mean? You might have also heard of big bang and how universe is expanding since. Tensors are an indispensable tool to understand what these mean mathematically. What does it really mean mathematically when people say Universe is expanding? Another popular example where tensors come up often is Quantum Mechanics(QM) and Quantum Computing(QC) , In QM, the mystical concept of Quantum Superposition where in a particles can remain in multiple states until it is observed is essential linear combination of tensors which we will be cover later. Quantum Superposition: Is the cat dead or alive? You may have heard that nothing travels faster than speed of light, yet if you separate two particles whose states are entangled, simple act of observing one instantaneously affects the other. Again, this is mathematically represented by tensor product which we will cover as well later in this series. \"Spooky action at a distance\"- Einstein. Huh? There is probably nothing more intriguing and fascinating than the unintuitive world of small scale (QM) as well as mind warping idea that time isn't same for everyone or that our universe is expanding. Tensors is a beautiful theory that allows one to study the underlying (complicated) Geometry of Space-time, something that one cannot get just by watching popular science articles or documentaries on these topics. Alfred North Whitehead said The idea that physicists would in future have to study the theory of tensors created real panic amongst them following the first announcement that Eistein's predictions had been verified. However, GTR and QM are not the only subjects tensors are used in. Tensors are prevalent in many domains of engineering and science, and good understanding of tensors would build a solid foundation for learning numerous other STEM subjects.","title":"Why would you want to study Tensor Algebra?"},{"location":"TensorAlgebra/Chap1/#what-are-tensors","text":"It is hard to explain what Tensor is. Different people tend to give different definition of tensor. And whats worse is that most of them are partially correct, but none completely. There are generally three ways to think about Tensor. Tensors are multi-dimensional arrays. An array is a list of numbers. If you replicate that list multiple times, like a table or MS excel, than it is called a 2-D array and so on and so forth. For example, you may have heard of popular scientific computation library called TensorFlow , which is all about manipulation of multi-dimensional arrays. Tensors are characterised by rank, as follows: Scalar (rank 0 tensor): [5], [1], [2.5], [\\pi] [5], [1], [2.5], [\\pi] Vector (rank 1 tensor): [1, 2, 3] [1, 2, 3] , [0.1, 3, \\sqrt{2}] [0.1, 3, \\sqrt{2}] Matrix (rank 2 tensor): \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} , \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} So on for rank 3, 4, ... This definition is incorrect Because while, Tensors can be represented as multi-dimensional arrays , tensors as concept are much more than just bunch of numbers. While calling multidimensional arrays tensor is generally accepted in Machine Learning community, it is not strictly correct. Tensors have geometrical meaning which is not apparent in this definition. Tensor is an object that is invariant under a change of coordinates . Under this definition, Tensors as object do not change when viewed from different coordinate system. As the choice of coordinate system changes (say Castersian to Polar), the representation of the same tensor in terms of the coordinate does change, however. The change is predictible allowing us to go back and forth from one coordinate system to another using known transformation rules.","title":"What are Tensors?"}]}