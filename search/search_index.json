{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"If you can\u2019t explain it simply, you don\u2019t understand it well enough. - Albert Einstein The purpose of this blog is two fold Learn better by following Feynman Technique Organize plethora of information I deem worth preserving for purpose of knowledge sharing and recall. The blog is broadly organized into following cateories: Tensor Algebra \u00b6 A short course (WIP) to build solid foundation in Tensor Algebra, which is a prerequisite to Tensor Calculus, which itself is a prerequisite to Differential Geometry, which in turn forms the mathematical language for Einstein's General Theory of Relativity. Machine Learning \u00b6 Different Machine learning topics, including an archive of interesting papers . Contact \u00b6 I am always looking for like-minded people and chances are, if you are reading this blog, you are one of them. If you are interested in giving feedback, provide new or discuss existing content, or just want to say hi, you can reach out to me at Email: abhi at abhitopia . com Twitter: abhitopia","title":""},{"location":"#tensor-algebra","text":"A short course (WIP) to build solid foundation in Tensor Algebra, which is a prerequisite to Tensor Calculus, which itself is a prerequisite to Differential Geometry, which in turn forms the mathematical language for Einstein's General Theory of Relativity.","title":"Tensor Algebra"},{"location":"#machine-learning","text":"Different Machine learning topics, including an archive of interesting papers .","title":"Machine Learning"},{"location":"#contact","text":"I am always looking for like-minded people and chances are, if you are reading this blog, you are one of them. If you are interested in giving feedback, provide new or discuss existing content, or just want to say hi, you can reach out to me at Email: abhi at abhitopia . com Twitter: abhitopia","title":"Contact"},{"location":"ML/reasoning_fundamentals/","text":"Reasoning about nature of Reasoning \u00b6 This is messy idea soup, and not meant for public consumption Knowledge Representation \u00b6 Comparison to word embeddings which do not account for context. The idea is that words are just symbols and the context determines which entities they are mapped to. This is also the reason that current neural architectures are not interpretable because we do not form embeddings of relations but instead of symbols. In this new paradigm, we must form embeddings for relations and embeddings Reasoning \u00b6 Intelligent system \u00b6 Learning Intelligent system \u00b6 Knowledge is embedded in analogies or relations. Based on above premise, with enough text data, one shall be able to extract a pretty good (and intelligent) description of world and reason about it. All intelligent arguments can be inferred from basic building block relations and set of primitive entities. \u00b6 Knowledge is then a set of primitive entities and corresponding relations. This need not be a fully connected graph. A complete knowlege system is one, where one can reason about any two entities in the entity space. Reasoning is then defined as 1. input -> entity_1 2. relation(entity_2) -> entity_2 Properties of relations \u00b6 They are very much like in maths R(A, B) -> {true| false} R(A) -> B R^(B) -> A Also, given A, B -> one should be able to find the relation (important for interpretability) Note: this does not mean that R^ must exist as a primitive relation. A reasoning system must be able to construct all entities from the primitive entities using convex combination. Imagine a spiders web. If primitive entities are represented at the outer edges of the web, then all the space of new (non-primitive) entities can be created as a linear combination of primitive entities. Implementation ideas \u00b6 |E| -> |R| number of possible arguments can then be |R|^(number of steps of reasoning). ( Connection with group theory?? ) One can argue that given the amount of relations and entities, any argument can be made with reason. Reasoning is then an optimization process to find the shortest relation between two (new or primitive) entities. This idea can also be used to determine when to stop reasoning. Role of context (temporal information, and symbols) \u00b6 context is then used solely to map the symbols (or any input modality) into corresponding entities. Attributes of Learning system \u00b6 learning system must be able to learn primitive entities and relations. learn the mapping function learning system must be able to contruct new relations (to reduce the amount of computation) Once set of primitive relations and primitive entities are acquired, along with the mapping (symbol, context) -> entity mapping function, a learning system should be very sample efficient in acquiring new concepts. Learning a task \u00b6 is tantamount to forming more dense relations amount context/task dependent entities. A > B, B > C => A > C Entities in this respect are just variables. It is the job of mapping function to map symbols to these variables. Learning \u00b6 relations that most used can follow hebbian learning rule, and thus are not the first one to be overwritten. adding new relations (must take some computation), that is an analogy must be made. a new relation (think more) is a composition of premitive relations??? thus a same inference can be obtained by either more computation by applying consequtive compositional relations on entities. Or a shortcut may be formed or repeated application (hebbian learning again?) optimization \u00b6 use forward prop to form new relations? can forward prop be a one step optimization process to find the analogies? Role of memory? \u00b6 Can some emtities or relation be more active due to priming effect of forward processing? Fast Weights? Perhaps priming can activate all the entities and relations connected by few steps to the last used entities and relations. demo \u00b6 Theorem proving? Sample Efficiency Interpretabilty? also consider adding optimization techniques like \u00b6 relative position for text (transformer-xl) TPR can be a good choice for representing entities and relations","title":"Reasoning about nature of reasoning"},{"location":"ML/reasoning_fundamentals/#reasoning-about-nature-of-reasoning","text":"This is messy idea soup, and not meant for public consumption","title":"Reasoning about nature of Reasoning"},{"location":"ML/reasoning_fundamentals/#knowledge-representation","text":"Comparison to word embeddings which do not account for context. The idea is that words are just symbols and the context determines which entities they are mapped to. This is also the reason that current neural architectures are not interpretable because we do not form embeddings of relations but instead of symbols. In this new paradigm, we must form embeddings for relations and embeddings","title":"Knowledge Representation"},{"location":"ML/reasoning_fundamentals/#reasoning","text":"","title":"Reasoning"},{"location":"ML/reasoning_fundamentals/#intelligent-system","text":"","title":"Intelligent system"},{"location":"ML/reasoning_fundamentals/#learning-intelligent-system","text":"Knowledge is embedded in analogies or relations. Based on above premise, with enough text data, one shall be able to extract a pretty good (and intelligent) description of world and reason about it.","title":"Learning Intelligent system"},{"location":"ML/reasoning_fundamentals/#all-intelligent-arguments-can-be-inferred-from-basic-building-block-relations-and-set-of-primitive-entities","text":"Knowledge is then a set of primitive entities and corresponding relations. This need not be a fully connected graph. A complete knowlege system is one, where one can reason about any two entities in the entity space. Reasoning is then defined as 1. input -> entity_1 2. relation(entity_2) -> entity_2","title":"All intelligent arguments can be inferred from basic building block relations and set of primitive entities."},{"location":"ML/reasoning_fundamentals/#properties-of-relations","text":"They are very much like in maths R(A, B) -> {true| false} R(A) -> B R^(B) -> A Also, given A, B -> one should be able to find the relation (important for interpretability) Note: this does not mean that R^ must exist as a primitive relation. A reasoning system must be able to construct all entities from the primitive entities using convex combination. Imagine a spiders web. If primitive entities are represented at the outer edges of the web, then all the space of new (non-primitive) entities can be created as a linear combination of primitive entities.","title":"Properties of relations"},{"location":"ML/reasoning_fundamentals/#implementation-ideas","text":"|E| -> |R| number of possible arguments can then be |R|^(number of steps of reasoning). ( Connection with group theory?? ) One can argue that given the amount of relations and entities, any argument can be made with reason. Reasoning is then an optimization process to find the shortest relation between two (new or primitive) entities. This idea can also be used to determine when to stop reasoning.","title":"Implementation ideas"},{"location":"ML/reasoning_fundamentals/#role-of-context-temporal-information-and-symbols","text":"context is then used solely to map the symbols (or any input modality) into corresponding entities.","title":"Role of context (temporal information, and symbols)"},{"location":"ML/reasoning_fundamentals/#attributes-of-learning-system","text":"learning system must be able to learn primitive entities and relations. learn the mapping function learning system must be able to contruct new relations (to reduce the amount of computation) Once set of primitive relations and primitive entities are acquired, along with the mapping (symbol, context) -> entity mapping function, a learning system should be very sample efficient in acquiring new concepts.","title":"Attributes of Learning system"},{"location":"ML/reasoning_fundamentals/#learning-a-task","text":"is tantamount to forming more dense relations amount context/task dependent entities. A > B, B > C => A > C Entities in this respect are just variables. It is the job of mapping function to map symbols to these variables.","title":"Learning a task"},{"location":"ML/reasoning_fundamentals/#learning","text":"relations that most used can follow hebbian learning rule, and thus are not the first one to be overwritten. adding new relations (must take some computation), that is an analogy must be made. a new relation (think more) is a composition of premitive relations??? thus a same inference can be obtained by either more computation by applying consequtive compositional relations on entities. Or a shortcut may be formed or repeated application (hebbian learning again?)","title":"Learning"},{"location":"ML/reasoning_fundamentals/#optimization","text":"use forward prop to form new relations? can forward prop be a one step optimization process to find the analogies?","title":"optimization"},{"location":"ML/reasoning_fundamentals/#role-of-memory","text":"Can some emtities or relation be more active due to priming effect of forward processing? Fast Weights? Perhaps priming can activate all the entities and relations connected by few steps to the last used entities and relations.","title":"Role of memory?"},{"location":"ML/reasoning_fundamentals/#demo","text":"Theorem proving? Sample Efficiency Interpretabilty?","title":"demo"},{"location":"ML/reasoning_fundamentals/#also-consider-adding-optimization-techniques-like","text":"relative position for text (transformer-xl) TPR can be a good choice for representing entities and relations","title":"also consider adding optimization techniques like"},{"location":"ML/Concepts/fast_weights/","text":"I read a sunday classic References \u00b6 Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets.","title":"Fast-Weights in RNN, from first principles"},{"location":"ML/Concepts/fast_weights/#references","text":"Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets.","title":"References"},{"location":"ML/Concepts/misc/","text":"NLP benchmark \u00b6 NLP Progress website Tranformer XL \u00b6 Code and pretrained model Paper AWD-LSTMS \u00b6 This is a great blog on the underlying concept and should help implement the same in trudle. It would be also good to see how awd-lstm compare with attentive awd-lstms. Blog RNN training methods and corresponding computational complexity \u00b6 Paper Short term memory for RNNs \u00b6 This paper propose by Jimmy Ba and Hinton, proposes an old idea of using Hebbian like learning rule on 2 nd order TPR representations to augment traditional RNN to incoorporate temporary, short-term memory. Paper Fast weight memories Video by Jimmy Video by Hinton My two cents \u00b6 Can this be combined with Neural Ordinary Differential equation to determine automatically determine the length of inner loop? Can decay parameter be learning, than have constant exponentail rate decay? It will be interesting to see how large scape Ulmfit like language model trained using truncated BPTT will do. If it works, it should be faster (and real time) during inference compared to attentional variants including Transformer XL. Symbolic Neural Reasoning using TPR \u00b6 This follows on Paul Smolensky work on symbolic reasoning using TPR and gives an end-to-end learning framework using Fast-Weights update of third order TPRs. Very interesting paper, which scope to build upon further. Paper My two cents \u00b6 Authors haven't been able to integrate it to a full fledged LSTM (or unable to make it work). The idea is novel and needs further investigation. Graph Neural Networks \u00b6 Paper Paper Blog Neural and Symbolic Reasoning \u00b6 Workshop Course FOL Theorem Proving Dataset HolStep Dataset DeepMath - Deep Sequence Models for Premise Selection My two cents \u00b6 Theorem proving and theorem validation is epitomy of symbolic reasoning. It would be interesting to see how the Fast-weights and TPR variants above do on these tasks. Logic Tensor Network \u00b6 The paper doesn't as good of a justice to the theory they develop in the tutorial. They lay the foundation of first order (fuzzy) logic in tensors. The tutorial is also a good introduction to fuzzy logic. Tutorial, Code, Notebooks Model confidence recalibration \u00b6 I stumbled upon this blog post when I noticed that after training my model for longer (and consequently obtaining higher accuracy), the confidence thresholding didn't work as well. This also hijacked the highly confident beams during beam search. The post is baed on a nicely written article, link below. Paper Pytorch Code My two cents \u00b6 This has not been tried in NLP settings. I am in process of implementing it now as part of my work at True AI. I shall post the results after.","title":"Paper Collection"},{"location":"ML/Concepts/misc/#nlp-benchmark","text":"NLP Progress website","title":"NLP benchmark"},{"location":"ML/Concepts/misc/#tranformer-xl","text":"Code and pretrained model Paper","title":"Tranformer XL"},{"location":"ML/Concepts/misc/#awd-lstms","text":"This is a great blog on the underlying concept and should help implement the same in trudle. It would be also good to see how awd-lstm compare with attentive awd-lstms. Blog","title":"AWD-LSTMS"},{"location":"ML/Concepts/misc/#rnn-training-methods-and-corresponding-computational-complexity","text":"Paper","title":"RNN training methods and corresponding computational complexity"},{"location":"ML/Concepts/misc/#short-term-memory-for-rnns","text":"This paper propose by Jimmy Ba and Hinton, proposes an old idea of using Hebbian like learning rule on 2 nd order TPR representations to augment traditional RNN to incoorporate temporary, short-term memory. Paper Fast weight memories Video by Jimmy Video by Hinton","title":"Short term memory for RNNs"},{"location":"ML/Concepts/misc/#my-two-cents","text":"Can this be combined with Neural Ordinary Differential equation to determine automatically determine the length of inner loop? Can decay parameter be learning, than have constant exponentail rate decay? It will be interesting to see how large scape Ulmfit like language model trained using truncated BPTT will do. If it works, it should be faster (and real time) during inference compared to attentional variants including Transformer XL.","title":"My two cents"},{"location":"ML/Concepts/misc/#symbolic-neural-reasoning-using-tpr","text":"This follows on Paul Smolensky work on symbolic reasoning using TPR and gives an end-to-end learning framework using Fast-Weights update of third order TPRs. Very interesting paper, which scope to build upon further. Paper","title":"Symbolic Neural Reasoning using TPR"},{"location":"ML/Concepts/misc/#my-two-cents_1","text":"Authors haven't been able to integrate it to a full fledged LSTM (or unable to make it work). The idea is novel and needs further investigation.","title":"My two cents"},{"location":"ML/Concepts/misc/#graph-neural-networks","text":"Paper Paper Blog","title":"Graph Neural Networks"},{"location":"ML/Concepts/misc/#neural-and-symbolic-reasoning","text":"Workshop Course FOL Theorem Proving Dataset HolStep Dataset DeepMath - Deep Sequence Models for Premise Selection","title":"Neural and Symbolic Reasoning"},{"location":"ML/Concepts/misc/#my-two-cents_2","text":"Theorem proving and theorem validation is epitomy of symbolic reasoning. It would be interesting to see how the Fast-weights and TPR variants above do on these tasks.","title":"My two cents"},{"location":"ML/Concepts/misc/#logic-tensor-network","text":"The paper doesn't as good of a justice to the theory they develop in the tutorial. They lay the foundation of first order (fuzzy) logic in tensors. The tutorial is also a good introduction to fuzzy logic. Tutorial, Code, Notebooks","title":"Logic Tensor Network"},{"location":"ML/Concepts/misc/#model-confidence-recalibration","text":"I stumbled upon this blog post when I noticed that after training my model for longer (and consequently obtaining higher accuracy), the confidence thresholding didn't work as well. This also hijacked the highly confident beams during beam search. The post is baed on a nicely written article, link below. Paper Pytorch Code","title":"Model confidence recalibration"},{"location":"ML/Concepts/misc/#my-two-cents_3","text":"This has not been tried in NLP settings. I am in process of implementing it now as part of my work at True AI. I shall post the results after.","title":"My two cents"},{"location":"ML/PaperSummaries/symbolic_dl/","text":"Link to article I must admit that I discovered this article trying to build my original ideas, and I was pleasantly surprised on how it overlapped 80%-90% on what I had come to conclude even before reading this. It is reassuring that to know that people who are far more qualified, and who have spent way more time than I have on this topic have come to similar conclusion (albeit with more references). Self adulation aside, lets get to business. Deep Learning \u00b6 Shortcoming in Deep Learning \u00b6 They categorize (with examples) which are fairly common and intuitive with current Deep Learning method. Data Inefficiency : Need no less than a simulator for Deep RL and millions of samples in old-fashioned DL. Poor Generalization : In ability to transfer or reuse learning to a (even slightly) different domain. Lack of Interpretability : Black box phenomenon. Strengths of DL \u00b6 Data driven automatic representation (and compute) learning : Throw in huge amount of data, and a suitable optimizer and wait. Symbolic AI \u00b6 Most predominant until 1980s, in symbolic AI, representations are typically propositional in character, and assert that certain relations hold between certain objects, while each reasoning step computes a further set of relations that follow from those already established, according to a formally specified set of inference rules. As reminder (and mathematically speaking), a proposition is a statement that evaluates to either True or False. Predicate is a function or mapping that takes variables produce propositional output. A relation is a predicate of two variables. Shortcoming of Symbolic AI \u00b6 Hand crafted representation : It is not clear how the representations can be learned from data and have traditionally been hand crafted to be used. This is known as symbol grounding problem . The contrast with DL is obvious. Strengths of Symbolic AI \u00b6 Strengths typically align with shortcomings of DL. Data efficiency : thanks to their declarative nature, symbolic representations lend themselves to re-use in multiple tasks, which promotes data efficiency. There is mostly no training needed, as the rules are often handcrafted. Cross-domain generalization : Symbolic representations tend to be high-level and abstract, which facilitates generalisation. As long as symbols are mapped to correct variables, and problem is correctly formulation in Symbolic AI language. Interpretable : Because of their language-like, propositional character, symbolic representations are amenable to human understanding. The predetermined rules of inference can be examined by humans. WIP, more to follow. Compositionality \u00b6 Show that this can be done using Tensors Relational Representation \u00b6 Can be done using Tensors It is easy to show that each layer of feed-forward network is than one function.","title":"Reconciling deep learning with symbolic artificial intelligence"},{"location":"ML/PaperSummaries/symbolic_dl/#deep-learning","text":"","title":"Deep Learning"},{"location":"ML/PaperSummaries/symbolic_dl/#shortcoming-in-deep-learning","text":"They categorize (with examples) which are fairly common and intuitive with current Deep Learning method. Data Inefficiency : Need no less than a simulator for Deep RL and millions of samples in old-fashioned DL. Poor Generalization : In ability to transfer or reuse learning to a (even slightly) different domain. Lack of Interpretability : Black box phenomenon.","title":"Shortcoming in Deep Learning"},{"location":"ML/PaperSummaries/symbolic_dl/#strengths-of-dl","text":"Data driven automatic representation (and compute) learning : Throw in huge amount of data, and a suitable optimizer and wait.","title":"Strengths of DL"},{"location":"ML/PaperSummaries/symbolic_dl/#symbolic-ai","text":"Most predominant until 1980s, in symbolic AI, representations are typically propositional in character, and assert that certain relations hold between certain objects, while each reasoning step computes a further set of relations that follow from those already established, according to a formally specified set of inference rules. As reminder (and mathematically speaking), a proposition is a statement that evaluates to either True or False. Predicate is a function or mapping that takes variables produce propositional output. A relation is a predicate of two variables.","title":"Symbolic AI"},{"location":"ML/PaperSummaries/symbolic_dl/#shortcoming-of-symbolic-ai","text":"Hand crafted representation : It is not clear how the representations can be learned from data and have traditionally been hand crafted to be used. This is known as symbol grounding problem . The contrast with DL is obvious.","title":"Shortcoming of Symbolic AI"},{"location":"ML/PaperSummaries/symbolic_dl/#strengths-of-symbolic-ai","text":"Strengths typically align with shortcomings of DL. Data efficiency : thanks to their declarative nature, symbolic representations lend themselves to re-use in multiple tasks, which promotes data efficiency. There is mostly no training needed, as the rules are often handcrafted. Cross-domain generalization : Symbolic representations tend to be high-level and abstract, which facilitates generalisation. As long as symbols are mapped to correct variables, and problem is correctly formulation in Symbolic AI language. Interpretable : Because of their language-like, propositional character, symbolic representations are amenable to human understanding. The predetermined rules of inference can be examined by humans. WIP, more to follow.","title":"Strengths of Symbolic AI"},{"location":"ML/PaperSummaries/symbolic_dl/#compositionality","text":"Show that this can be done using Tensors","title":"Compositionality"},{"location":"ML/PaperSummaries/symbolic_dl/#relational-representation","text":"Can be done using Tensors It is easy to show that each layer of feed-forward network is than one function.","title":"Relational Representation"},{"location":"Maths/fuzzy_logic/","text":"Logic (from the Greek \"logos\", which has a variety of meanings including word, thought, idea, argument, account, reason or principle) is the study of reasoning, or the study of the principles and criteria of valid inference and demonstration. It attempts to distinguish good reasoning from bad reasoning. Conventional Logic typically deals with binary truth values {True/False}. However, real world (and human feelings for certain) seldom works in binary. As humans, we constantly make choices on vague or imprecise judgements. For example, one may not know for sure if it is going to rain (unless of course you are living in London, when not a day goes by without raining) and yet she may decide to carry an umbrella, there by making binary decision based on non-binary reasoning. For computers to be able to reason in real world and to solve real world problems, they must be equipped to make sense of such fuzzy data which is neither completely true, nor completely false. That is where Fuzzy Logic comes to the rescue. It is a branch of many-valued symbolic logic which has comparative notion of truth, syntax, semantics, axiomatization, truth-preserving, completeness, etc. Broadly speaking, Fuzzy logic serves mainly as apparatus for fuzzy control, analysis of vagueness in natural language and several other application domains. It is one of the techniques of soft-computing, i.e. computational methods tolerant to suboptimality and impreciseness (vagueness) and giving quick, simple and sufficiently good solutions. In this blog post, I will summarise Fuzzy logic and more importantly Fuzzy First Order Logic (FFOL). This post is not intended to be a rigorous mathematical treatment of the subject but rather a cheat-sheet that can be consulted to revise and reference important concepts. Crisp and Fuzzy Sets \u00b6 In classical mathematics, one deals with collection of objects called sets . It is usually convenient to fix some universe U U in which every set is assumed to be included 1 . Then a set A A can be thought of as a function on U U which takes a values of 1 1 for objects that belong to A A , 0 0 otherwise. Such a function is called the characteristic function of A A , \\chi_A(.) \\chi_A(.) : \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} Obviously, this implies that there exist a bijective mapping between characteristic function and maps. In fact, this is how sets are defined in ZFC Set Theory. Crisp Set Let X X be the set of all real numbers between 0 and 10 and let A = [5, 9] A = [5, 9] be the subset of X X of real numbers between 5 and 9. This results in the following figure: Fuzzy sets generalise this definition, allowing elements to belong to a given set with a certain degree. Instead of considering characteristic functions with value in {0, 1} {0, 1} , we consider now functions valued in the interval [0, 1] [0, 1] . A fuzzy subset F F of a set U U is a function \\mu_F(\u00b7) \\mu_F(\u00b7) assigning to every element x \\in U x \\in U the degree of membership of x x to F F : \\mu_F : U \\rightarrow [0, 1] \\mu_F : U \\rightarrow [0, 1] Fuzzy Set Let, as above, U be the set of real numbers between 1 and 10. A description of the fuzzy set of real numbers close to 7 could be given by the following figure: Operations between crisp sets \u00b6 In classical set theory there are some basic operations defined over sets. Let U U be a set and 2^U 2^U (also known as power set) be the set of all subsets of U. Since there exists, a bijection between sets and the corresponding characteristic function, one could equivalently say that a power set, 2^U 2^U is a set defined by a characteristic function \\chi_{2^U}: U \\rightarrow \\{0, 1\\} \\chi_{2^U}: U \\rightarrow \\{0, 1\\} . The operation of union, intersection and complement are defined in the following ways: Operation Crisp Set Characteristic Function Union A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) Intersection A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) Compliment \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x) \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x) Operations between fuzzy sets \u00b6 While concept of binary membership ( \\in \\in ) does not exist in fuzzy sets, we can draw parallels to the characteristic function for crisp set and apply that to membership functions of fuzzy set analogously to derive set operations. Operation Fuzzy Set Membership Function Union \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) Intersection \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) Compliment \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) Fuzzy Set Operations Let A A and Bs Bs be fuzzy subsets of U U given by membership functions \\mu_F \\mu_F and \\mu_S \\mu_S : Union Intersection Compliment \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) References \u00b6 Neural-Symbolic Learning and Reasoning with Constraints Tutorial at IEEE IJCNN 2018 Refer to ZFC Set theory and famous Russel's paradox for more rigorous arguments. \u21a9","title":"Fuzzy First Order Logic"},{"location":"Maths/fuzzy_logic/#crisp-and-fuzzy-sets","text":"In classical mathematics, one deals with collection of objects called sets . It is usually convenient to fix some universe U U in which every set is assumed to be included 1 . Then a set A A can be thought of as a function on U U which takes a values of 1 1 for objects that belong to A A , 0 0 otherwise. Such a function is called the characteristic function of A A , \\chi_A(.) \\chi_A(.) : \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} \\chi_A(x) := \\begin{cases} 1 & x\\in A \\\\ 0 & x \\notin A \\end{cases} Obviously, this implies that there exist a bijective mapping between characteristic function and maps. In fact, this is how sets are defined in ZFC Set Theory. Crisp Set Let X X be the set of all real numbers between 0 and 10 and let A = [5, 9] A = [5, 9] be the subset of X X of real numbers between 5 and 9. This results in the following figure: Fuzzy sets generalise this definition, allowing elements to belong to a given set with a certain degree. Instead of considering characteristic functions with value in {0, 1} {0, 1} , we consider now functions valued in the interval [0, 1] [0, 1] . A fuzzy subset F F of a set U U is a function \\mu_F(\u00b7) \\mu_F(\u00b7) assigning to every element x \\in U x \\in U the degree of membership of x x to F F : \\mu_F : U \\rightarrow [0, 1] \\mu_F : U \\rightarrow [0, 1] Fuzzy Set Let, as above, U be the set of real numbers between 1 and 10. A description of the fuzzy set of real numbers close to 7 could be given by the following figure:","title":"Crisp and Fuzzy Sets"},{"location":"Maths/fuzzy_logic/#operations-between-crisp-sets","text":"In classical set theory there are some basic operations defined over sets. Let U U be a set and 2^U 2^U (also known as power set) be the set of all subsets of U. Since there exists, a bijection between sets and the corresponding characteristic function, one could equivalently say that a power set, 2^U 2^U is a set defined by a characteristic function \\chi_{2^U}: U \\rightarrow \\{0, 1\\} \\chi_{2^U}: U \\rightarrow \\{0, 1\\} . The operation of union, intersection and complement are defined in the following ways: Operation Crisp Set Characteristic Function Union A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} A \\cup B = \\{x \\mid x \\in A \\ or\\ x \\in B\\} \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cup B}(x) = max(\\chi_A(x), \\chi_B(x)) Intersection A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} A \\cap B = \\{x \\mid x \\in A \\ and\\ x \\in B\\} \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) \\chi_{A \\cap B}(x) = min(\\chi_A(x), \\chi_B(x)) Compliment \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\bar{A} = \\{ x \\in U \\mid x \\notin A \\} \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x) \\chi_{\\bar{A}}(x) = 1 - \\chi_A(x)","title":"Operations between crisp sets"},{"location":"Maths/fuzzy_logic/#operations-between-fuzzy-sets","text":"While concept of binary membership ( \\in \\in ) does not exist in fuzzy sets, we can draw parallels to the characteristic function for crisp set and apply that to membership functions of fuzzy set analogously to derive set operations. Operation Fuzzy Set Membership Function Union \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) Intersection \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) Compliment \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) Fuzzy Set Operations Let A A and Bs Bs be fuzzy subsets of U U given by membership functions \\mu_F \\mu_F and \\mu_S \\mu_S : Union Intersection Compliment \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cup B}(x) = max(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{A \\cap B}(x) = min(\\mu_A(x), \\mu_B(x)) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x) \\mu_{\\bar{A}}(x) = 1 - \\mu_A(x)","title":"Operations between fuzzy sets"},{"location":"Maths/fuzzy_logic/#references","text":"Neural-Symbolic Learning and Reasoning with Constraints Tutorial at IEEE IJCNN 2018 Refer to ZFC Set theory and famous Russel's paradox for more rigorous arguments. \u21a9","title":"References"},{"location":"TensorAlgebra/Chap1/","text":"Chapter 1 \u00b6 Welcome to this blog series on Tensor Algebra. This blog series is my attempt to explain Tensor Algebra as I learnt from various sources over the Internet. This chapter is mostly to motivate why you might want to study Tensor Algebra. Pre-requisite \u00b6 This blog series assumes a familiarity with Linear Algebra (concepts like matrix-multiplication, dot products, linear combination, etc.) This blog should be particularly helpful for those who never had formal college degree in Physics and as such find it hard to delve into certain subjects where Tensors are pre-requisite. No knowledge of calculus is needed to study Tensor Algebra. I may do a follow up series on Tensor Calculus though. Why would you want to study Tensor Algebra? \u00b6 Most people first hear about Tensor Algebra when they start out their journey to delve deeper into the General Theory of Relatively (GTR) . In GTR, Eistein shows how space-time forms a single object which is curved by massive objects ( and other crazy ideas like blackholes and gravitational waves). Space time curves near massive objects as predicted by General Theory of Relativity, but what does that even mean? You might have also heard of big bang and how universe is expanding since. Tensors are an indispensable tool to understand what these mean mathematically. What does it really mean mathematically when people say Universe is expanding? Another popular example where tensors come up often is Quantum Mechanics(QM) and Quantum Computing(QC) , In QM, the mystical concept of Quantum Superposition where in a particles can remain in multiple states until it is observed is essential linear combination of tensors which we will be cover later. Quantum Superposition: Is the cat dead or alive? You may have heard that nothing travels faster than speed of light, yet if you separate two particles whose states are entangled, simple act of observing one instantaneously affects the other. Again, this is mathematically represented by tensor product which we will cover as well later in this series. \"Spooky action at a distance\"- Einstein. Huh? There is probably nothing more intriguing and fascinating than the unintuitive world of small scale (QM) as well as mind warping idea that time isn't same for everyone or that our universe is expanding. Tensors is a beautiful theory that allows one to study the underlying (complicated) Geometry of Space-time, something that one cannot get just by watching popular science articles or documentaries on these topics. Alfred North Whitehead said The idea that physicists would in future have to study the theory of tensors created real panic amongst them following the first announcement that Eistein's predictions had been verified. However, GTR and QM are not the only subjects tensors are used in. Tensors are prevalent in many domains of engineering and science, and good understanding of tensors would build a solid foundation for learning numerous other STEM subjects. What are Tensors? \u00b6 It is hard to explain what Tensor is. Different people tend to give different definition of tensor. And whats worse is that most of them are partially correct, but none completely. There are generally three ways to think about Tensor. Tensors are multi-dimensional arrays. An array is a list of numbers. If you replicate that list multiple times, like a table or MS excel, than it is called a 2-D array and so on and so forth. For example, you may have heard of popular scientific computation library called TensorFlow , which is all about manipulation of multi-dimensional arrays. Tensors are characterised by rank, as follows: Scalar (rank 0 tensor): [5], [1], [2.5], [\\pi] [5], [1], [2.5], [\\pi] Vector (rank 1 tensor): [1, 2, 3] [1, 2, 3] , [0.1, 3, \\sqrt{2}] [0.1, 3, \\sqrt{2}] Matrix (rank 2 tensor): \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} , \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} So on for rank 3, 4, ... This definition is incorrect Because while, Tensors can be represented as multi-dimensional arrays , tensors as concept are much more than just bunch of numbers. While calling multidimensional arrays tensor is generally accepted in Machine Learning community, it is not strictly correct. Tensors have geometrical meaning which is not apparent in this definition. Tensor is an object that is invariant under a change of coordinates . Under this definition, Tensors as object do not change when viewed from different coordinate system. As the choice of coordinate system changes (say Castersian to Polar), the representation of the same tensor in terms of the coordinate does change, however. The change is predictible allowing us to go back and forth from one coordinate system to another using known transformation rules.","title":"Chapter 1"},{"location":"TensorAlgebra/Chap1/#chapter-1","text":"Welcome to this blog series on Tensor Algebra. This blog series is my attempt to explain Tensor Algebra as I learnt from various sources over the Internet. This chapter is mostly to motivate why you might want to study Tensor Algebra.","title":"Chapter 1"},{"location":"TensorAlgebra/Chap1/#pre-requisite","text":"This blog series assumes a familiarity with Linear Algebra (concepts like matrix-multiplication, dot products, linear combination, etc.) This blog should be particularly helpful for those who never had formal college degree in Physics and as such find it hard to delve into certain subjects where Tensors are pre-requisite. No knowledge of calculus is needed to study Tensor Algebra. I may do a follow up series on Tensor Calculus though.","title":"Pre-requisite"},{"location":"TensorAlgebra/Chap1/#why-would-you-want-to-study-tensor-algebra","text":"Most people first hear about Tensor Algebra when they start out their journey to delve deeper into the General Theory of Relatively (GTR) . In GTR, Eistein shows how space-time forms a single object which is curved by massive objects ( and other crazy ideas like blackholes and gravitational waves). Space time curves near massive objects as predicted by General Theory of Relativity, but what does that even mean? You might have also heard of big bang and how universe is expanding since. Tensors are an indispensable tool to understand what these mean mathematically. What does it really mean mathematically when people say Universe is expanding? Another popular example where tensors come up often is Quantum Mechanics(QM) and Quantum Computing(QC) , In QM, the mystical concept of Quantum Superposition where in a particles can remain in multiple states until it is observed is essential linear combination of tensors which we will be cover later. Quantum Superposition: Is the cat dead or alive? You may have heard that nothing travels faster than speed of light, yet if you separate two particles whose states are entangled, simple act of observing one instantaneously affects the other. Again, this is mathematically represented by tensor product which we will cover as well later in this series. \"Spooky action at a distance\"- Einstein. Huh? There is probably nothing more intriguing and fascinating than the unintuitive world of small scale (QM) as well as mind warping idea that time isn't same for everyone or that our universe is expanding. Tensors is a beautiful theory that allows one to study the underlying (complicated) Geometry of Space-time, something that one cannot get just by watching popular science articles or documentaries on these topics. Alfred North Whitehead said The idea that physicists would in future have to study the theory of tensors created real panic amongst them following the first announcement that Eistein's predictions had been verified. However, GTR and QM are not the only subjects tensors are used in. Tensors are prevalent in many domains of engineering and science, and good understanding of tensors would build a solid foundation for learning numerous other STEM subjects.","title":"Why would you want to study Tensor Algebra?"},{"location":"TensorAlgebra/Chap1/#what-are-tensors","text":"It is hard to explain what Tensor is. Different people tend to give different definition of tensor. And whats worse is that most of them are partially correct, but none completely. There are generally three ways to think about Tensor. Tensors are multi-dimensional arrays. An array is a list of numbers. If you replicate that list multiple times, like a table or MS excel, than it is called a 2-D array and so on and so forth. For example, you may have heard of popular scientific computation library called TensorFlow , which is all about manipulation of multi-dimensional arrays. Tensors are characterised by rank, as follows: Scalar (rank 0 tensor): [5], [1], [2.5], [\\pi] [5], [1], [2.5], [\\pi] Vector (rank 1 tensor): [1, 2, 3] [1, 2, 3] , [0.1, 3, \\sqrt{2}] [0.1, 3, \\sqrt{2}] Matrix (rank 2 tensor): \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix} , \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix} So on for rank 3, 4, ... This definition is incorrect Because while, Tensors can be represented as multi-dimensional arrays , tensors as concept are much more than just bunch of numbers. While calling multidimensional arrays tensor is generally accepted in Machine Learning community, it is not strictly correct. Tensors have geometrical meaning which is not apparent in this definition. Tensor is an object that is invariant under a change of coordinates . Under this definition, Tensors as object do not change when viewed from different coordinate system. As the choice of coordinate system changes (say Castersian to Polar), the representation of the same tensor in terms of the coordinate does change, however. The change is predictible allowing us to go back and forth from one coordinate system to another using known transformation rules.","title":"What are Tensors?"}]}